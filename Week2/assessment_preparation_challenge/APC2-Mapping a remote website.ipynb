{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assessment Preparation Challenge 2: Mapping a Remote Website\n",
    "\n",
    "---\n",
    "\n",
    "## About This Challenge\n",
    "\n",
    "| | |\n",
    "|---|---|\n",
    "| **Purpose** | Practice web crawling skills on real websites |\n",
    "| **Graded?** | No - this is purely for your benefit |\n",
    "| **Solutions provided?** | No - similar tasks appear in the final assessment |\n",
    "| **Difficulty** | Medium (builds on Week 2 lab exercises) |\n",
    "| **Prerequisites** | Complete Week2-exercise.ipynb first |\n",
    "| **Time estimate** | 30-45 minutes |\n",
    "\n",
    "---\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "By completing this challenge, you will:\n",
    "\n",
    "1. **Adapt** local file crawlers to work with remote URLs\n",
    "2. **Handle** different URL formats (relative vs absolute)\n",
    "3. **Filter** links to stay within a specific domain\n",
    "4. **Limit** crawl depth to avoid infinite loops\n",
    "5. **Create** readable network visualizations\n",
    "6. **Analyze** website structure using PageRank and HITS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Setup\n",
    "\n",
    "Run this cell first to import all required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to load all dependencies\n",
    "# (You can run cells with Shift+Enter or the RUN button above)\n",
    "\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import pprint as pp\n",
    "import requests\n",
    "\n",
    "print(\"All libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Step 1: Understanding URL Formats\n",
    "\n",
    "Before crawling remote sites, you need to understand how URLs work. This is **different** from local files!\n",
    "\n",
    "## Types of Links You'll Encounter\n",
    "\n",
    "| Link Type | Example | What It Means |\n",
    "|-----------|---------|---------------|\n",
    "| **Absolute** | `https://example.com/about` | Complete URL - use as-is |\n",
    "| **Relative** | `/about` | Path only - needs domain prepended |\n",
    "| **Fragment** | `/page#section` | Links to section - remove `#section` |\n",
    "| **Query** | `/page?id=123` | Has parameters - often remove |\n",
    "| **External** | `https://twitter.com/user` | Different domain - usually skip |\n",
    "| **Email** | `mailto:info@example.com` | Not a page - always skip |\n",
    "\n",
    "## Practice: Test Your Understanding\n",
    "\n",
    "If you're on the page `https://example.com/products/shoes`, what would these links become?\n",
    "\n",
    "1. `/contact` → ?\n",
    "2. `../about` → ?\n",
    "3. `https://facebook.com` → ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to see answers</summary>\n",
    "\n",
    "1. `/contact` → `https://example.com/contact` (relative path from root)\n",
    "2. `../about` → `https://example.com/about` (go up one directory)\n",
    "3. `https://facebook.com` → SKIP! (external domain)\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Step 2: Understanding the Helper Functions\n",
    "\n",
    "We provide three helper functions to handle URL processing. **Read through each one carefully** - understanding them is key to completing the challenge!\n",
    "\n",
    "## Function 1: `domain_only(url)`\n",
    "\n",
    "**Purpose**: Extract just the domain from a full URL.\n",
    "\n",
    "**Why we need it**: To check if a link stays within our target website.\n",
    "\n",
    "**Examples**:\n",
    "- `\"https://bbc.co.uk/news/weather\"` → `\"bbc.co.uk\"`\n",
    "- `\"https://www.example.com/page\"` → `\"www.example.com\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def domain_only(full_url):\n",
    "    \"\"\"\n",
    "    Extract the domain from a full URL.\n",
    "    \n",
    "    Example: 'https://bbc.co.uk/news' -> 'bbc.co.uk'\n",
    "    \"\"\"\n",
    "    # Remove the protocol (https:// or http://)\n",
    "    without_https = full_url.lstrip(\"https\").lstrip(\"http\").lstrip(\"://\")\n",
    "    \n",
    "    # Find where the path starts (first /)\n",
    "    end_of_domain = without_https.find(\"/\") if \"/\" in without_https else len(without_https)\n",
    "    \n",
    "    # Return just the domain part\n",
    "    return without_https[0:end_of_domain]\n",
    "\n",
    "# Test the function - all should print \"tests passed\"\n",
    "assert domain_only(\"https://bbc.co.uk/\") == \"bbc.co.uk\"\n",
    "assert domain_only(\"https://bbc.co.uk\") == \"bbc.co.uk\"\n",
    "assert domain_only(\"https://bbc.co.uk/news\") == \"bbc.co.uk\"\n",
    "assert domain_only(\"http://twitter.com/account\") == \"twitter.com\"\n",
    "print(\"domain_only() tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function 2: `protocol_only(url)`\n",
    "\n",
    "**Purpose**: Get `http://` or `https://` from a URL.\n",
    "\n",
    "**Why we need it**: To reconstruct full URLs from relative links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def protocol_only(full_url):\n",
    "    \"\"\"\n",
    "    Extract the protocol (http:// or https://) from a URL.\n",
    "    \n",
    "    Example: 'https://bbc.co.uk/' -> 'https://'\n",
    "    \"\"\"\n",
    "    if \"https://\" in full_url:\n",
    "        return \"https://\" \n",
    "    elif \"http://\" in full_url:\n",
    "        return \"http://\" \n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "# Test the function\n",
    "assert protocol_only(\"https://bbc.co.uk/\") == \"https://\"\n",
    "assert protocol_only(\"http://bbc.co.uk\") == \"http://\"\n",
    "assert protocol_only(\"bbc.co.uk/news\") == \"\"\n",
    "print(\"protocol_only() tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function 3: `cleanup_the_link(page_url, link_address)`\n",
    "\n",
    "**Purpose**: Convert any link format to a clean, absolute URL.\n",
    "\n",
    "**What it does**:\n",
    "1. Handles relative links (adds domain)\n",
    "2. Filters external links (optional)\n",
    "3. Removes fragments (`#section`)\n",
    "4. Removes query strings (`?param=value`)\n",
    "5. Removes trailing slashes for consistency\n",
    "\n",
    "**This is the most important function!** Study it carefully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_the_link(page_url, link_address, keep_external_links=False):\n",
    "    \"\"\"\n",
    "    Clean up a link found on a page and convert to absolute URL.\n",
    "    \n",
    "    Args:\n",
    "        page_url: The URL of the page where we found this link\n",
    "        link_address: The href value of the link\n",
    "        keep_external_links: If False, replace external links with page_url\n",
    "    \n",
    "    Returns:\n",
    "        Cleaned absolute URL\n",
    "    \"\"\"\n",
    "    \n",
    "    # Handle relative links (no http in them)\n",
    "    if \"http\" not in link_address: \n",
    "        # Internal link like '/about' or 'products.html'\n",
    "        # Combine protocol + domain + link\n",
    "        link_address = protocol_only(page_url) + domain_only(page_url) + link_address\n",
    "        \n",
    "    else:\n",
    "        # Absolute link - check if it's external\n",
    "        if domain_only(link_address) != domain_only(page_url) and keep_external_links == False:\n",
    "            # External link to different domain - replace with current page\n",
    "            link_address = page_url\n",
    "    \n",
    "    # Skip email links\n",
    "    if \"mailto\" in link_address:\n",
    "        link_address = page_url\n",
    "    \n",
    "    # Remove fragment identifiers (#section)\n",
    "    if \"#\" in link_address:\n",
    "        link_address = link_address[0:link_address.find(\"#\")]\n",
    "    \n",
    "    # Remove query parameters (?id=123)\n",
    "    if \"?\" in link_address:\n",
    "        link_address = link_address[0:link_address.find(\"?\")]\n",
    "        \n",
    "    # Remove trailing slash for consistency\n",
    "    link_address = link_address.rstrip(\"/\")\n",
    "        \n",
    "    return link_address\n",
    "\n",
    "# Test the function\n",
    "assert cleanup_the_link(\"https://bbc.co.uk/\", \"/news\") == \"https://bbc.co.uk/news\"\n",
    "assert cleanup_the_link(\"https://bbc.co.uk\", \"/news\") == \"https://bbc.co.uk/news\"\n",
    "assert cleanup_the_link(\"https://bbc.co.uk/news\", \"/weather\") == \"https://bbc.co.uk/weather\"\n",
    "assert cleanup_the_link(\"https://twitter.com\", \"https://bbc.co.uk/\") == \"https://twitter.com\"  # External filtered\n",
    "assert cleanup_the_link(\"https://twitter.com\", \"https://bbc.co.uk/\", True) == \"https://bbc.co.uk\"  # External kept\n",
    "assert cleanup_the_link(\"https://bbc.co.uk/news\", \"/weather#footer\") == \"https://bbc.co.uk/weather\"  # Fragment removed\n",
    "assert cleanup_the_link(\"https://bbc.co.uk/news\", \"/mailto:info@bbc.co.uk\") == \"https://bbc.co.uk/news\"  # Email skipped\n",
    "assert cleanup_the_link(\"https://bbc.co.uk/news\", \"/weather?city=edinburgh\") == \"https://bbc.co.uk/weather\"  # Query removed\n",
    "print(\"cleanup_the_link() tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Step 3: Build Your Crawler Step-by-Step\n",
    "\n",
    "Now we'll build the crawler incrementally. Complete each sub-step before moving to the next.\n",
    "\n",
    "## Step 3a: Visit One Page and Get All Links\n",
    "\n",
    "First, let's just visit the starting page and see what links we find."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visit_page_and_return_dictionary(page_url, keep_external_links=False):\n",
    "    \"\"\"\n",
    "    Visit a web page and return all cleaned links.\n",
    "    \n",
    "    Args:\n",
    "        page_url: URL to visit\n",
    "        keep_external_links: Whether to include links to other domains\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with 'address' and 'links_to' keys\n",
    "    \"\"\"\n",
    "    link_urls = []\n",
    "    print(f\"Looking for links in {page_url}\")\n",
    "    \n",
    "    try:\n",
    "        # Request the page with a timeout\n",
    "        html_of_website = requests.get(page_url, timeout=10).content.decode()    \n",
    "    except Exception as e:\n",
    "        # If something went wrong, return empty links\n",
    "        print(f\"  Error: {e}\")\n",
    "        return {'address': page_url, 'links_to': []}\n",
    "    \n",
    "    # Parse HTML and find all links\n",
    "    soup = BeautifulSoup(html_of_website, 'html.parser')\n",
    "    links = soup.find_all('a')\n",
    "    \n",
    "    for link in links:\n",
    "        link_url = link.get('href', default=page_url)\n",
    "        \n",
    "        # Clean up the link\n",
    "        link_url = cleanup_the_link(page_url, link_url, keep_external_links)\n",
    "        link_urls.append(link_url)\n",
    "        \n",
    "    # Remove duplicates\n",
    "    link_urls = list(set(link_urls))\n",
    "        \n",
    "    return {'address': page_url, 'links_to': link_urls}\n",
    "\n",
    "# Test with a simple website\n",
    "test_result = visit_page_and_return_dictionary(\"https://www.marysmilkbar.com/yourvisit\")\n",
    "print(f\"\\nFound {len(test_result['links_to'])} links\")\n",
    "pp.pprint(test_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3b: Add the Crawl Loop\n",
    "\n",
    "Now let's add the frontier management to crawl multiple pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_website(starting_website, keep_external_links=False, max_pages=20):\n",
    "    \"\"\"\n",
    "    Crawl a website starting from a given URL.\n",
    "    \n",
    "    Args:\n",
    "        starting_website: URL to start crawling\n",
    "        keep_external_links: Whether to follow external links\n",
    "        max_pages: Maximum pages to crawl (to prevent infinite crawling)\n",
    "    \n",
    "    Returns:\n",
    "        List of page info dictionaries\n",
    "    \"\"\"\n",
    "    pages_we_visited = []\n",
    "    pages_to_visit = [starting_website] \n",
    "    pages_scraped_info = []\n",
    "\n",
    "    # Keep crawling until no more pages or we hit the limit\n",
    "    while len(pages_to_visit) > 0 and len(pages_we_visited) < max_pages:\n",
    "        # Get next page from the frontier\n",
    "        next_page_to_visit = pages_to_visit.pop() \n",
    "        \n",
    "        # Visit the page\n",
    "        page_info = visit_page_and_return_dictionary(next_page_to_visit, keep_external_links)\n",
    "        pages_scraped_info.append(page_info)\n",
    "\n",
    "        # Mark as visited\n",
    "        pages_we_visited.append(page_info['address']) \n",
    "        \n",
    "        # Add new links to frontier\n",
    "        for link_url in page_info['links_to']:\n",
    "            if link_url not in pages_we_visited and link_url not in pages_to_visit:\n",
    "                pages_to_visit.append(link_url)\n",
    "\n",
    "        # Remove duplicates from frontier\n",
    "        pages_to_visit = list(set(pages_to_visit))\n",
    "        \n",
    "        # Be polite - add a small delay\n",
    "        time.sleep(0.5)\n",
    "\n",
    "    print(f\"\\nCrawl complete! Visited {len(pages_we_visited)} pages.\")\n",
    "    return pages_scraped_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3c: Test with a Small Website\n",
    "\n",
    "Let's test our crawler with a small, simple website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crawl a small website (limited to 10 pages)\n",
    "scraped_pages = analyse_website(\"https://www.marysmilkbar.com/\", max_pages=10)\n",
    "\n",
    "# Show the results\n",
    "print(\"\\nPages discovered:\")\n",
    "for page in scraped_pages:\n",
    "    print(f\"  {page['address']} -> {len(page['links_to'])} links\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Step 4: Creating and Visualizing the Graph\n",
    "\n",
    "Now let's create a NetworkX graph and visualize it.\n",
    "\n",
    "## Basic Graph Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_site_info(pages_info):\n",
    "    \"\"\"\n",
    "    Create and display a graph from crawled page data.\n",
    "    \"\"\"\n",
    "    graph = nx.DiGraph()\n",
    "\n",
    "    # Add edges for each link\n",
    "    for page in pages_info:\n",
    "        link_origin = page['address']\n",
    "        all_links = page['links_to']\n",
    "        for link_destination in all_links:\n",
    "            graph.add_edge(link_origin, link_destination)\n",
    "\n",
    "    # Draw the graph\n",
    "    positions = nx.spring_layout(graph)\n",
    "    nx.draw(graph, positions, with_labels=True, font_size=6) \n",
    "    plt.show()\n",
    "    \n",
    "    return graph\n",
    "\n",
    "# Create the basic graph\n",
    "graph = graph_site_info(scraped_pages)\n",
    "print(\"\\nNote: The graph might look messy - we'll improve it next!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making the Graph Readable\n",
    "\n",
    "The default graph is often messy. Here are tips to improve it:\n",
    "\n",
    "### Tip 1: Shorten URL Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shorten_url(url, max_length=25):\n",
    "    \"\"\"\n",
    "    Make URL labels more readable by removing common prefixes.\n",
    "    \"\"\"\n",
    "    short = url.replace('https://www.', '').replace('https://', '').replace('http://', '')\n",
    "    if len(short) > max_length:\n",
    "        short = short[:max_length] + '...'\n",
    "    return short\n",
    "\n",
    "# Test it\n",
    "print(shorten_url(\"https://www.marysmilkbar.com/yourvisit\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tip 2: Create an Improved Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def improved_graph_visualization(pages_info):\n",
    "    \"\"\"\n",
    "    Create a cleaner, more readable graph visualization.\n",
    "    \"\"\"\n",
    "    # Create the graph with shortened labels\n",
    "    graph = nx.DiGraph()\n",
    "    label_map = {}  # Map full URLs to short labels\n",
    "    \n",
    "    for page in pages_info:\n",
    "        origin = page['address']\n",
    "        origin_short = shorten_url(origin)\n",
    "        label_map[origin_short] = origin\n",
    "        \n",
    "        for dest in page['links_to']:\n",
    "            dest_short = shorten_url(dest)\n",
    "            label_map[dest_short] = dest\n",
    "            graph.add_edge(origin_short, dest_short)\n",
    "    \n",
    "    # Calculate PageRank for node sizing\n",
    "    if graph.number_of_nodes() > 0:\n",
    "        pageranks = nx.pagerank(graph)\n",
    "        sizes = [pageranks.get(n, 0.01) * 5000 for n in graph.nodes()]\n",
    "    else:\n",
    "        sizes = [300] * graph.number_of_nodes()\n",
    "    \n",
    "    # Create larger figure\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    \n",
    "    # Try different layouts - kamada_kawai often looks cleaner\n",
    "    try:\n",
    "        positions = nx.kamada_kawai_layout(graph)\n",
    "    except:\n",
    "        positions = nx.spring_layout(graph, k=2, seed=42)\n",
    "    \n",
    "    # Draw with styling\n",
    "    nx.draw_networkx_nodes(graph, positions, \n",
    "                           node_size=sizes, \n",
    "                           node_color='lightblue',\n",
    "                           alpha=0.8)\n",
    "    nx.draw_networkx_edges(graph, positions, \n",
    "                           edge_color='gray', \n",
    "                           arrows=True,\n",
    "                           arrowsize=10,\n",
    "                           alpha=0.5)\n",
    "    nx.draw_networkx_labels(graph, positions, font_size=7)\n",
    "    \n",
    "    plt.title(\"Website Link Structure (node size = PageRank)\")\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return graph, pageranks\n",
    "\n",
    "# Try the improved visualization\n",
    "graph, pageranks = improved_graph_visualization(scraped_pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Step 5: Analyze Your Results\n",
    "\n",
    "After crawling, answer these analysis questions about the website you crawled.\n",
    "\n",
    "## PageRank Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display PageRank scores\n",
    "print(\"PageRank Scores (sorted):\")\n",
    "print(\"=\" * 50)\n",
    "for page, score in sorted(pageranks.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"{page:40} {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HITS Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate HITS scores\n",
    "hubs, authorities = nx.hits(graph)\n",
    "\n",
    "print(\"Hub Scores (pages that link to many):\")\n",
    "print(\"-\" * 50)\n",
    "for page, score in sorted(hubs.items(), key=lambda x: x[1], reverse=True)[:5]:\n",
    "    print(f\"{page:40} {score:.4f}\")\n",
    "\n",
    "print(\"\\nAuthority Scores (pages linked to by many):\")\n",
    "print(\"-\" * 50)\n",
    "for page, score in sorted(authorities.items(), key=lambda x: x[1], reverse=True)[:5]:\n",
    "    print(f\"{page:40} {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions to Answer\n",
    "\n",
    "Think about and answer these questions in the cells below:\n",
    "\n",
    "1. **Structure**: How many pages did you discover? Are there clusters of related pages?\n",
    "\n",
    "2. **Dead Ends**: Which pages have no outgoing links? Why might a website have dead-end pages?\n",
    "\n",
    "3. **Central Pages**: Which page has the highest PageRank? Does this make sense for the website's purpose?\n",
    "\n",
    "4. **Hub vs Authority**: Which page is the best hub? Which is the best authority? Are they the same or different?\n",
    "\n",
    "5. **Business Insight**: If this were your company's website, what would you recommend improving based on the link structure?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Write your answers here:*\n",
    "\n",
    "1. **Structure**: \n",
    "\n",
    "2. **Dead Ends**: \n",
    "\n",
    "3. **Central Pages**: \n",
    "\n",
    "4. **Hub vs Authority**: \n",
    "\n",
    "5. **Business Insight**: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Your Challenge: Try Different Websites\n",
    "\n",
    "Now it's your turn! Try crawling different websites and analyzing their structure.\n",
    "\n",
    "## Recommended Websites for Practice\n",
    "\n",
    "| Website | Difficulty | Notes |\n",
    "|---------|------------|-------|\n",
    "| https://www.marysmilkbar.com/ | Easy | Small, static site |\n",
    "| https://www.soderberg.uk/ | Easy | Small bakery site |\n",
    "| https://quotes.toscrape.com/ | Easy | Designed for scraping practice |\n",
    "| https://books.toscrape.com/ | Medium | More pages, pagination |\n",
    "| https://www.edinburghshogmanay.com/ | Medium | Event website |\n",
    "| https://gauss.world/ | Medium | Personal website |\n",
    "| https://bagrow.com/ | Medium | Academic website |\n",
    "\n",
    "## Websites to Avoid\n",
    "\n",
    "- **Large sites** (BBC, Amazon) - too many pages, will take forever\n",
    "- **JavaScript-heavy sites** - require Playwright, not just requests\n",
    "- **Sites with aggressive rate limiting** - might block you\n",
    "- **Sites that explicitly disallow crawling** - check robots.txt!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CHALLENGE: Crawl and analyze a different website\n",
    "# ======================================================\n",
    "\n",
    "# Choose a website from the list above (or find your own small site)\n",
    "my_website = \"https://www.soderberg.uk/\"  # Change this!\n",
    "\n",
    "# Crawl it (adjust max_pages as needed)\n",
    "my_pages = analyse_website(my_website, max_pages=15)\n",
    "\n",
    "# Visualize\n",
    "my_graph, my_pageranks = improved_graph_visualization(my_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze your results\n",
    "print(\"Top 5 pages by PageRank:\")\n",
    "for page, score in sorted(my_pageranks.items(), key=lambda x: x[1], reverse=True)[:5]:\n",
    "    print(f\"  {page}: {score:.4f}\")\n",
    "\n",
    "# Your analysis here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Bonus: Try with External Links\n",
    "\n",
    "See what happens when you allow the crawler to follow external links!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caution: This will crawl across domains!\n",
    "# Keep max_pages LOW to avoid crawling the entire internet\n",
    "\n",
    "pages_with_external = analyse_website(\"https://www.soderberg.uk/\", \n",
    "                                       keep_external_links=True, \n",
    "                                       max_pages=15)\n",
    "\n",
    "# Notice how it expands to social media links, etc.\n",
    "improved_graph_visualization(pages_with_external)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "Congratulations! You've completed the APC2 challenge. You now know how to:\n",
    "\n",
    "1. Handle different URL formats when crawling remote websites\n",
    "2. Filter links to stay within a domain\n",
    "3. Limit crawling to avoid infinite loops\n",
    "4. Create readable graph visualizations\n",
    "5. Analyze website structure using PageRank and HITS\n",
    "\n",
    "These skills will be useful for the final assessment!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
