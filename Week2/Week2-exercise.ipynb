{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2: Exercises - Web Crawling, Link Analysis & PageRank\n",
    "\n",
    "**Web and Social Network Analytics**\n",
    "\n",
    "---\n",
    "\n",
    "**Instructions**: Complete each exercise in the provided code cells. Use the hints if you get stuck - they progressively reveal more help.\n",
    "\n",
    "**Disclaimer**: This educational content is provided for instructional purposes only. Always respect website terms of service and robots.txt files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Run this cell first to import all required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import os\n",
    "import time\n",
    "import pathlib\n",
    "from urllib.request import urlopen\n",
    "import pprint as pp\n",
    "\n",
    "# Web scraping\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# Graph analysis\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data handling\n",
    "import pandas as pd\n",
    "\n",
    "print('All libraries imported successfully!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for local files (provided for you)\n",
    "def create_local_file_address(folder, file):\n",
    "    \"\"\"Create a file:// URL that works on any operating system.\"\"\"\n",
    "    file_address = os.path.join(os.getcwd(), folder, file)\n",
    "    with_schema = pathlib.Path(file_address).as_uri()\n",
    "    return with_schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 1: Link Extraction Basics (Easy)\n",
    "\n",
    "**Task**: Extract all hyperlinks from the `home.html` page in the `demowebsite` folder.\n",
    "\n",
    "**Expected Output**: A list of 4 links: `['team.html', 'news.html', 'business_deals.html', 'shop.html']`\n",
    "\n",
    "**Skills Practiced**:\n",
    "- Opening local HTML files with `urlopen()`\n",
    "- Using BeautifulSoup to find elements\n",
    "- Extracting `href` attribute values\n",
    "\n",
    "---\n",
    "\n",
    "<details>\n",
    "<summary>Hint 1: Opening local files</summary>\n",
    "\n",
    "Use the provided helper function:\n",
    "```python\n",
    "file_path = create_local_file_address(\"demowebsite\", \"home.html\")\n",
    "html = urlopen(file_path)\n",
    "```\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Hint 2: Finding all links</summary>\n",
    "\n",
    "Parse with BeautifulSoup and find anchor tags:\n",
    "```python\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "links = soup.find_all('a')\n",
    "```\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Hint 3: Extracting href values</summary>\n",
    "\n",
    "Loop through links and get the href attribute:\n",
    "```python\n",
    "for link in links:\n",
    "    href = link.get('href')  # or link['href']\n",
    "    print(href)\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Your code here\n",
    "# ===========================\n",
    "\n",
    "# Step 1: Create the file path for home.html\n",
    "\n",
    "\n",
    "# Step 2: Open and parse the HTML file\n",
    "\n",
    "\n",
    "# Step 3: Find all anchor (<a>) tags\n",
    "\n",
    "\n",
    "# Step 4: Extract and store href values in a list\n",
    "link_urls = []\n",
    "\n",
    "\n",
    "# Step 5: Print the results\n",
    "print(f\"Found {len(link_urls)} links:\")\n",
    "print(link_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 2: Shingling and Jaccard Similarity (Easy-Medium)\n",
    "\n",
    "**Task**: Calculate the Jaccard similarity between two documents using k=2 word shingles.\n",
    "\n",
    "**Documents**:\n",
    "- Document A: `\"The quick brown fox jumps\"`\n",
    "- Document B: `\"The quick red fox leaps\"`\n",
    "\n",
    "**Expected Output**:\n",
    "- Show shingles for each document\n",
    "- Show intersection and union\n",
    "- Calculate Jaccard similarity (should be 1/7 = 0.143)\n",
    "\n",
    "**Skills Practiced**:\n",
    "- Implementing the shingling algorithm\n",
    "- Set operations in Python\n",
    "- Jaccard similarity calculation\n",
    "\n",
    "---\n",
    "\n",
    "<details>\n",
    "<summary>Hint 1: Creating shingles</summary>\n",
    "\n",
    "Break text into overlapping word pairs:\n",
    "```python\n",
    "def create_shingles(text, k=2):\n",
    "    words = text.lower().split()\n",
    "    shingles = set()\n",
    "    for i in range(len(words) - k + 1):\n",
    "        shingle = ' '.join(words[i:i+k])\n",
    "        shingles.add(shingle)\n",
    "    return shingles\n",
    "```\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Hint 2: Set operations</summary>\n",
    "\n",
    "Python set operations:\n",
    "- Intersection: `set1 & set2` or `set1.intersection(set2)`\n",
    "- Union: `set1 | set2` or `set1.union(set2)`\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Your code here\n",
    "# ===========================\n",
    "\n",
    "# Step 1: Define the documents\n",
    "doc_a = \"The quick brown fox jumps\"\n",
    "doc_b = \"The quick red fox leaps\"\n",
    "\n",
    "# Step 2: Implement the create_shingles function\n",
    "def create_shingles(text, k=2):\n",
    "    \"\"\"Create k-word shingles from text.\"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    pass\n",
    "\n",
    "# Step 3: Create shingles for both documents\n",
    "\n",
    "\n",
    "# Step 4: Implement Jaccard similarity function\n",
    "def jaccard_similarity(set1, set2):\n",
    "    \"\"\"Calculate Jaccard similarity between two sets.\"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    pass\n",
    "\n",
    "# Step 5: Calculate and display results\n",
    "print(\"Document A shingles:\")\n",
    "# print(...)\n",
    "\n",
    "print(\"\\nDocument B shingles:\")\n",
    "# print(...)\n",
    "\n",
    "print(\"\\nIntersection:\")\n",
    "# print(...)\n",
    "\n",
    "print(\"\\nUnion:\")\n",
    "# print(...)\n",
    "\n",
    "print(\"\\nJaccard Similarity:\")\n",
    "# print(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 3: Building a Complete Web Crawler (Medium)\n",
    "\n",
    "**Task**: Implement a crawler that maps all pages in the `demowebsite` folder and stores the link structure.\n",
    "\n",
    "**Requirements**:\n",
    "1. Start from `home.html`\n",
    "2. Visit all reachable pages (following links)\n",
    "3. Track which pages link to which\n",
    "4. Return a list of dictionaries: `[{'address': 'page.html', 'links_to': [...]}]`\n",
    "\n",
    "**Expected Output**: Complete map of the demowebsite structure (should discover all 8 HTML files reachable from home)\n",
    "\n",
    "**Skills Practiced**:\n",
    "- Frontier management (pages to visit vs visited)\n",
    "- Avoiding infinite loops\n",
    "- Structured data collection\n",
    "\n",
    "---\n",
    "\n",
    "<details>\n",
    "<summary>Hint 1: Data structures to use</summary>\n",
    "\n",
    "```python\n",
    "pages_visited = []        # Track what we've already processed\n",
    "pages_to_visit = ['home.html']  # The frontier\n",
    "pages_info = []           # Store structured results\n",
    "```\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Hint 2: Main loop structure</summary>\n",
    "\n",
    "```python\n",
    "while len(pages_to_visit) > 0:\n",
    "    current_page = pages_to_visit.pop()\n",
    "    # 1. Visit page and get links\n",
    "    # 2. Store structured data\n",
    "    # 3. Mark as visited\n",
    "    # 4. Add new links to frontier (if not visited)\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Your code here\n",
    "# ===========================\n",
    "\n",
    "def visit_page_and_return_dictionary(page_url):\n",
    "    \"\"\"Visit a local HTML page and return structured link data.\"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    # Should return: {'address': page_url, 'links_to': [list of links]}\n",
    "    pass\n",
    "\n",
    "# Initialize data structures\n",
    "starting_website = \"home.html\"\n",
    "pages_visited = []\n",
    "pages_to_visit = [starting_website]\n",
    "pages_info = []\n",
    "\n",
    "# Implement the crawl loop\n",
    "# YOUR CODE HERE\n",
    "\n",
    "\n",
    "# Display results\n",
    "print(f\"\\nCrawled {len(pages_visited)} pages:\")\n",
    "print(pages_visited)\n",
    "print(\"\\nComplete link structure:\")\n",
    "pp.pprint(pages_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 4: PageRank Calculation (Medium)\n",
    "\n",
    "**Task**: Using the crawler results from Exercise 3, build a NetworkX graph and calculate PageRank scores.\n",
    "\n",
    "**Requirements**:\n",
    "1. Create a directed graph (`nx.DiGraph()`) from your crawler data\n",
    "2. Calculate PageRank for all pages using `nx.pagerank()`\n",
    "3. Visualize the graph with node sizes proportional to PageRank\n",
    "4. Identify the highest and lowest PageRank pages\n",
    "\n",
    "**Expected Output**: \n",
    "- Graph visualization with varying node sizes\n",
    "- Sorted list of PageRank scores\n",
    "- Analysis: Which page is most \"important\"?\n",
    "\n",
    "**Skills Practiced**:\n",
    "- NetworkX graph construction\n",
    "- PageRank computation\n",
    "- Data visualization\n",
    "\n",
    "---\n",
    "\n",
    "<details>\n",
    "<summary>Hint 1: Creating the graph</summary>\n",
    "\n",
    "```python\n",
    "graph = nx.DiGraph()\n",
    "for page in pages_info:\n",
    "    for destination in page['links_to']:\n",
    "        graph.add_edge(page['address'], destination)\n",
    "```\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Hint 2: Calculating PageRank</summary>\n",
    "\n",
    "```python\n",
    "pageranks = nx.pagerank(graph)\n",
    "# Returns a dictionary: {'page.html': 0.123, ...}\n",
    "```\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Hint 3: Sizing nodes by PageRank</summary>\n",
    "\n",
    "```python\n",
    "sizes = [pageranks[node] * 5000 for node in graph.nodes()]\n",
    "nx.draw(graph, node_size=sizes, with_labels=True)\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4: Your code here\n",
    "# ===========================\n",
    "\n",
    "# Step 1: Create a directed graph from crawler data\n",
    "graph = nx.DiGraph()\n",
    "# YOUR CODE HERE\n",
    "\n",
    "\n",
    "# Step 2: Calculate PageRank\n",
    "# YOUR CODE HERE\n",
    "\n",
    "\n",
    "# Step 3: Print sorted PageRank scores\n",
    "print(\"PageRank Scores (highest to lowest):\")\n",
    "print(\"-\" * 40)\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Visualize the graph with node sizes based on PageRank\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "plt.title(\"DemoWebsite - Node Size = PageRank\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Analysis**: Which page has the highest PageRank? Why do you think that is?\n",
    "\n",
    "*Write your answer here:*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 5: HITS Algorithm Comparison (Medium-Hard)\n",
    "\n",
    "**Task**: Calculate HITS scores for the demowebsite graph and compare with PageRank results.\n",
    "\n",
    "**Requirements**:\n",
    "1. Calculate hub and authority scores using `nx.hits()`\n",
    "2. Create a comparison table: Page | PageRank | Hub Score | Authority Score\n",
    "3. Identify: Which page is the best hub? Which is the best authority?\n",
    "4. Explain why the rankings might differ from PageRank\n",
    "\n",
    "**Expected Output**:\n",
    "- HITS scores (hubs and authorities dictionaries)\n",
    "- Comparison DataFrame\n",
    "- Written analysis\n",
    "\n",
    "**Skills Practiced**:\n",
    "- HITS algorithm application\n",
    "- Comparing ranking algorithms\n",
    "- Analytical interpretation\n",
    "\n",
    "---\n",
    "\n",
    "<details>\n",
    "<summary>Hint 1: Calculate HITS</summary>\n",
    "\n",
    "```python\n",
    "hubs, authorities = nx.hits(graph)\n",
    "# Both are dictionaries like PageRank\n",
    "```\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Hint 2: Create comparison table</summary>\n",
    "\n",
    "```python\n",
    "data = []\n",
    "for page in graph.nodes():\n",
    "    data.append({\n",
    "        'Page': page,\n",
    "        'PageRank': pageranks[page],\n",
    "        'Hub': hubs[page],\n",
    "        'Authority': authorities[page]\n",
    "    })\n",
    "df = pd.DataFrame(data)\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 5: Your code here\n",
    "# ===========================\n",
    "\n",
    "# Step 1: Calculate HITS scores\n",
    "# YOUR CODE HERE\n",
    "\n",
    "\n",
    "# Step 2: Print hub scores (sorted)\n",
    "print(\"Hub Scores (highest to lowest):\")\n",
    "print(\"-\" * 40)\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Print authority scores (sorted)\n",
    "print(\"Authority Scores (highest to lowest):\")\n",
    "print(\"-\" * 40)\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Create comparison DataFrame\n",
    "# YOUR CODE HERE\n",
    "\n",
    "print(\"\\nComparison Table:\")\n",
    "# print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Analysis**: \n",
    "\n",
    "1. Which page is the best **hub**? Why?\n",
    "\n",
    "*Write your answer here:*\n",
    "\n",
    "2. Which page is the best **authority**? Why?\n",
    "\n",
    "*Write your answer here:*\n",
    "\n",
    "3. Why might HITS rankings differ from PageRank?\n",
    "\n",
    "*Write your answer here:*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Bonus Challenge: Remote Website Crawler\n",
    "\n",
    "**Task**: Adapt your crawler to work with a real (small) website instead of local files.\n",
    "\n",
    "**Suggested Websites** (small and stable):\n",
    "- https://quotes.toscrape.com (limited pages, designed for practice)\n",
    "- A small personal/academic website\n",
    "\n",
    "**Additional Requirements**:\n",
    "- Handle absolute vs relative URLs\n",
    "- Filter to stay within the same domain\n",
    "- Limit to maximum 10-15 pages\n",
    "- Add delay between requests (`time.sleep(1)`)\n",
    "- Handle errors gracefully (try/except)\n",
    "\n",
    "**Warning**: Always check robots.txt before crawling!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bonus Challenge: Your code here\n",
    "# ================================\n",
    "\n",
    "# Helper functions for URL handling\n",
    "def get_domain(url):\n",
    "    \"\"\"Extract domain from URL.\"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    pass\n",
    "\n",
    "def clean_url(base_url, link):\n",
    "    \"\"\"Convert relative URLs to absolute and clean up.\"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    pass\n",
    "\n",
    "def visit_remote_page(url):\n",
    "    \"\"\"Visit a remote URL and return links (with error handling).\"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    pass\n",
    "\n",
    "# Main crawler for remote websites\n",
    "def crawl_website(start_url, max_pages=10):\n",
    "    \"\"\"Crawl a website starting from start_url.\"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    pass\n",
    "\n",
    "# Test with quotes.toscrape.com\n",
    "# results = crawl_website(\"https://quotes.toscrape.com\", max_pages=10)\n",
    "# pp.pprint(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Submission Checklist\n",
    "\n",
    "Before submitting, verify:\n",
    "\n",
    "- [ ] Exercise 1: Extracted 4 links from home.html\n",
    "- [ ] Exercise 2: Correctly calculated Jaccard similarity (~0.143)\n",
    "- [ ] Exercise 3: Crawler discovered all reachable pages\n",
    "- [ ] Exercise 4: PageRank visualization and scores displayed\n",
    "- [ ] Exercise 5: HITS comparison table with analysis\n",
    "- [ ] All code cells run without errors\n",
    "- [ ] Written analyses provided where requested"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
