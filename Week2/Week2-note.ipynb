{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2: Web Crawling, Link Analysis & PageRank\n",
    "\n",
    "**Web and Social Network Analytics**\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this lab, you will be able to:\n",
    "\n",
    "1. **Build** a web crawler that respects robots.txt conventions\n",
    "2. **Extract** and analyze link structures using BeautifulSoup\n",
    "3. **Calculate** document similarity using shingling and Jaccard similarity\n",
    "4. **Construct** link graphs with NetworkX\n",
    "5. **Compute** PageRank and HITS scores for webpage importance\n",
    "6. **(Optional)** Use LLMs to summarize scraped content\n",
    "\n",
    "---\n",
    "\n",
    "**Disclaimer**: This educational content is provided for instructional purposes only. Always respect website terms of service, robots.txt files, and legal requirements when crawling. Be polite to servers by adding delays between requests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Run this cell first to import all required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import os\n",
    "import time\n",
    "import pathlib\n",
    "from urllib.request import urlopen\n",
    "from urllib.parse import urlparse\n",
    "import pprint as pp\n",
    "\n",
    "# Web scraping\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# Graph analysis\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data handling\n",
    "import pandas as pd\n",
    "\n",
    "print('All libraries imported successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: Introduction to Web Crawling\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Web Crawling vs Web Scraping\n",
    "\n",
    "These terms are often confused, but they serve different purposes:\n",
    "\n",
    "| Aspect | Web Crawling | Web Scraping |\n",
    "|--------|--------------|---------------|\n",
    "| **Purpose** | Discovering and indexing web content | Extracting specific data from pages |\n",
    "| **Process** | Navigating links, mapping site structure | Parsing HTML for targeted information |\n",
    "| **Usage** | Search engines, site mapping | Data analysis, research, monitoring |\n",
    "| **Output** | Links, metadata, page relationships | Structured data (tables, text, prices) |\n",
    "| **Focus** | Breadth (many pages) | Depth (specific content) |\n",
    "\n",
    "**This week**: We focus on **web crawling** - discovering how pages link to each other and analyzing that structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 The Web Crawler Process\n",
    "\n",
    "A web crawler (also called a spider or bot) follows this process:\n",
    "\n",
    "1. **Start** with a list of seed URLs (the \"frontier\")\n",
    "2. **Fetch** a page from the frontier\n",
    "3. **Parse** the page to extract all hyperlinks\n",
    "4. **Add** new links to the frontier (if not already visited)\n",
    "5. **Repeat** until frontier is empty or limit reached\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "- **Frontier**: Queue of URLs waiting to be crawled\n",
    "- **Visited set**: URLs already processed (to avoid duplicates)\n",
    "- **Politeness**: Delays between requests to respect servers\n",
    "- **robots.txt**: File that tells crawlers what they can/cannot access"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking robots.txt\n",
    "\n",
    "Before crawling any website, you should check its `robots.txt` file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Check a website's robots.txt\n",
    "def check_robots_txt(domain):\n",
    "    \"\"\"Fetch and display a website's robots.txt file.\"\"\"\n",
    "    url = f\"https://{domain}/robots.txt\"\n",
    "    try:\n",
    "        response = requests.get(url, timeout=5)\n",
    "        if response.status_code == 200:\n",
    "            print(f\"robots.txt for {domain}:\")\n",
    "            print(\"-\" * 40)\n",
    "            # Show first 500 characters\n",
    "            print(response.text[:500])\n",
    "            if len(response.text) > 500:\n",
    "                print(\"... (truncated)\")\n",
    "        else:\n",
    "            print(f\"No robots.txt found (status: {response.status_code})\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "# Try it with Google\n",
    "check_robots_txt(\"google.com\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Hands-On: Manual Link Mapping Exercise\n",
    "\n",
    "Before we automate crawling, let's understand it manually.\n",
    "\n",
    "### Task\n",
    "\n",
    "1. Open the `demowebsite` folder in your file browser\n",
    "2. Open `home.html` in a web browser\n",
    "3. Click through all the links and draw a map on paper showing:\n",
    "   - Which pages link to which\n",
    "   - Use arrows to show direction (A -> B means A links to B)\n",
    "\n",
    "### Questions to Answer\n",
    "\n",
    "1. Can you reach all HTML files in the folder by clicking from home.html?\n",
    "2. Which pages are \"dead ends\" (no outgoing links)?\n",
    "3. Which page is linked to by the most other pages?\n",
    "4. Which page has the most outgoing links?\n",
    "\n",
    "*Keep your diagram - we'll verify it with code later!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: Building a Simple Web Crawler\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Helper Function for Local Files\n",
    "\n",
    "Since we're practicing with local HTML files, we need a helper to create proper file URLs that work across operating systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_local_file_address(folder, file):\n",
    "    \"\"\"Create a file:// URL that works on any operating system.\"\"\"\n",
    "    file_address = os.path.join(os.getcwd(), folder, file)\n",
    "    with_schema = pathlib.Path(file_address).as_uri()\n",
    "    return with_schema\n",
    "\n",
    "# Test it\n",
    "test_url = create_local_file_address(\"demowebsite\", \"home.html\")\n",
    "print(f\"Local URL: {test_url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Extracting Links from a Single Page\n",
    "\n",
    "Let's write a function that visits a page and returns all the links it finds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visit_page_and_return_links(page_url):\n",
    "    \"\"\"Visit a local HTML page and return all links found.\"\"\"\n",
    "    # Create the full file path\n",
    "    page_url_full = create_local_file_address(\"demowebsite\", page_url)\n",
    "    print(f\"Looking for links in: {page_url}\")\n",
    "    \n",
    "    # Open and parse the page\n",
    "    html_of_website = urlopen(page_url_full)\n",
    "    soup = BeautifulSoup(html_of_website, 'html.parser')\n",
    "    \n",
    "    # Find all anchor tags and extract href values\n",
    "    links = soup.find_all('a')\n",
    "    link_urls = []\n",
    "    \n",
    "    for link in links:\n",
    "        href = link.get('href')  # Safer than link['href']\n",
    "        if href:\n",
    "            print(f\"  Found link: {link.text} -> {href}\")\n",
    "            link_urls.append(href)\n",
    "    \n",
    "    return link_urls\n",
    "\n",
    "# Test with home page\n",
    "starting_website = \"home.html\"\n",
    "found_links = visit_page_and_return_links(starting_website)\n",
    "print(f\"\\nTotal links found: {len(found_links)}\")\n",
    "print(f\"Links: {found_links}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Returning Structured Data\n",
    "\n",
    "For building a graph later, we need structured data. Let's modify our function to return a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of the structure we want:\n",
    "demo_page = {\n",
    "    \"address\": \"home.html\",\n",
    "    \"links_to\": [\"team.html\", \"news.html\", \"business_deals.html\", \"shop.html\"]\n",
    "}\n",
    "\n",
    "pp.pprint(demo_page)  # Pretty print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visit_page_and_return_dictionary(page_url):\n",
    "    \"\"\"Visit a local HTML page and return structured link data.\"\"\"\n",
    "    page_url_full = create_local_file_address(\"demowebsite\", page_url)\n",
    "    print(f\"Visiting: {page_url}\")\n",
    "    \n",
    "    html_of_website = urlopen(page_url_full)\n",
    "    soup = BeautifulSoup(html_of_website, 'html.parser')\n",
    "    \n",
    "    links = soup.find_all('a')\n",
    "    link_urls = []\n",
    "    for link in links:\n",
    "        href = link.get('href')\n",
    "        if href:\n",
    "            link_urls.append(href)\n",
    "    \n",
    "    # Return structured data\n",
    "    return {\n",
    "        'address': page_url,\n",
    "        'links_to': link_urls\n",
    "    }\n",
    "\n",
    "# Test it\n",
    "page_info = visit_page_and_return_dictionary(\"home.html\")\n",
    "print()\n",
    "pp.pprint(page_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Complete Crawler Implementation\n",
    "\n",
    "Now let's build the full crawler with frontier management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize our data structures\n",
    "starting_website = \"home.html\"\n",
    "\n",
    "pages_we_visited = []           # Pages we've already processed\n",
    "pages_to_visit = [starting_website]  # The frontier\n",
    "pages_scraped_info = []         # Store structured data for each page\n",
    "\n",
    "# Crawl until frontier is empty\n",
    "while len(pages_to_visit) > 0:\n",
    "    # Get next page from frontier\n",
    "    next_page = pages_to_visit.pop()\n",
    "    \n",
    "    # Visit and get link info\n",
    "    page_info = visit_page_and_return_dictionary(next_page)\n",
    "    pages_scraped_info.append(page_info)\n",
    "    \n",
    "    # Mark as visited\n",
    "    pages_we_visited.append(page_info['address'])\n",
    "    \n",
    "    # Add new links to frontier (if not visited)\n",
    "    for link_url in page_info['links_to']:\n",
    "        if link_url not in pages_we_visited and link_url not in pages_to_visit:\n",
    "            pages_to_visit.append(link_url)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"CRAWLING COMPLETE!\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nPages visited ({len(pages_we_visited)}): {pages_we_visited}\")\n",
    "print(f\"\\nRemaining in frontier: {pages_to_visit}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the complete scraped data\n",
    "print(\"Complete link structure:\")\n",
    "print(\"-\" * 50)\n",
    "pp.pprint(pages_scraped_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify Your Manual Diagram\n",
    "\n",
    "Compare the output above with the diagram you drew earlier. Does it match?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 3: Link Graphs and NetworkX\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Introduction to Directed Graphs\n",
    "\n",
    "A **directed graph (DiGraph)** is perfect for representing web links:\n",
    "- **Nodes** = web pages\n",
    "- **Edges** = hyperlinks (with direction: from -> to)\n",
    "\n",
    "Let's start with a simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple directed graph\n",
    "simple_graph = nx.DiGraph()\n",
    "\n",
    "# Add some edges (automatically creates nodes)\n",
    "simple_graph.add_edge('A', 'B')  # A links to B\n",
    "simple_graph.add_edge('A', 'C')  # A links to C\n",
    "simple_graph.add_edge('A', 'D')  # A links to D\n",
    "simple_graph.add_edge('C', 'B')  # C links to B\n",
    "\n",
    "# Calculate layout and draw\n",
    "positions = nx.spring_layout(simple_graph, seed=42)  # seed for reproducibility\n",
    "nx.draw(simple_graph, positions, with_labels=True, \n",
    "        node_color='lightblue', node_size=1000, \n",
    "        font_size=16, arrows=True, arrowsize=20)\n",
    "plt.title(\"Simple Directed Graph\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Notice the arrows showing link direction!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Converting Crawler Data to Graph\n",
    "\n",
    "Now let's convert our `pages_scraped_info` into a NetworkX graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create graph from our crawler data\n",
    "graph = nx.DiGraph()\n",
    "\n",
    "# For each page we scraped\n",
    "for page in pages_scraped_info:\n",
    "    origin = page['address']  # The page we visited\n",
    "    destinations = page['links_to']  # Pages it links to\n",
    "    \n",
    "    # Add an edge for each link\n",
    "    for dest in destinations:\n",
    "        graph.add_edge(origin, dest)\n",
    "\n",
    "print(f\"Graph has {graph.number_of_nodes()} nodes and {graph.number_of_edges()} edges\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Visualizing the Web Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a larger figure for better visibility\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Calculate layout\n",
    "positions = nx.spring_layout(graph, seed=2026, k=2)  # k controls spacing\n",
    "\n",
    "# Draw the graph\n",
    "nx.draw(graph, positions, \n",
    "        with_labels=True,\n",
    "        node_color='lightblue',\n",
    "        node_size=2000,\n",
    "        font_size=10,\n",
    "        arrows=True,\n",
    "        arrowsize=15,\n",
    "        edge_color='gray')\n",
    "\n",
    "plt.title(\"DemoWebsite Link Structure\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions to Consider\n",
    "\n",
    "Looking at the graph:\n",
    "1. Which page is the \"hub\" (most outgoing links)?\n",
    "2. Which page is the most linked-to (most incoming links)?\n",
    "3. Are there any dead ends (nodes with no outgoing arrows)?\n",
    "4. Can you reach every page from home.html?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 4: Similarity Detection with Shingling\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Why Detect Duplicates?\n",
    "\n",
    "**About 40% of the web is duplicate content!**\n",
    "\n",
    "This includes:\n",
    "- Copy-pasted articles across news sites\n",
    "- Product descriptions on multiple e-commerce platforms\n",
    "- Slightly modified spam content\n",
    "- Mirror sites and archived versions\n",
    "\n",
    "Search engines need efficient ways to detect duplicates and near-duplicates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Shingling: Breaking Text into Pieces\n",
    "\n",
    "**Shingling** (like roof shingles that overlap) breaks text into overlapping pieces called **k-shingles**.\n",
    "\n",
    "**Example** with k=2 (word-level):\n",
    "\n",
    "Text: `\"The quick brown fox\"`\n",
    "\n",
    "2-shingles: `{\"the quick\", \"quick brown\", \"brown fox\"}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_shingles(text, k=2):\n",
    "    \"\"\"Create k-word shingles from text.\n",
    "    \n",
    "    Args:\n",
    "        text: Input string\n",
    "        k: Number of words per shingle\n",
    "    \n",
    "    Returns:\n",
    "        Set of shingles\n",
    "    \"\"\"\n",
    "    # Normalize: lowercase and split into words\n",
    "    words = text.lower().split()\n",
    "    \n",
    "    # Create shingles\n",
    "    shingles = set()\n",
    "    for i in range(len(words) - k + 1):\n",
    "        shingle = ' '.join(words[i:i+k])\n",
    "        shingles.add(shingle)\n",
    "    \n",
    "    return shingles\n",
    "\n",
    "# Test it\n",
    "text = \"The quick brown fox\"\n",
    "shingles = create_shingles(text, k=2)\n",
    "print(f\"Text: '{text}'\")\n",
    "print(f\"2-shingles: {shingles}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Jaccard Similarity\n",
    "\n",
    "**Jaccard Similarity** measures how similar two sets are:\n",
    "\n",
    "$$J(A, B) = \\frac{|A \\cap B|}{|A \\cup B|} = \\frac{\\text{Intersection}}{\\text{Union}}$$\n",
    "\n",
    "- **J = 1.0**: Identical sets\n",
    "- **J = 0.0**: No overlap\n",
    "- **J > 0.8**: Often considered \"duplicate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(set1, set2):\n",
    "    \"\"\"Calculate Jaccard similarity between two sets.\"\"\"\n",
    "    intersection = set1 & set2  # Elements in both\n",
    "    union = set1 | set2         # Elements in either\n",
    "    \n",
    "    if len(union) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    return len(intersection) / len(union)\n",
    "\n",
    "# Example\n",
    "set_a = {1, 2, 3, 4}\n",
    "set_b = {3, 4, 5, 6}\n",
    "\n",
    "print(f\"Set A: {set_a}\")\n",
    "print(f\"Set B: {set_b}\")\n",
    "print(f\"Intersection: {set_a & set_b}\")\n",
    "print(f\"Union: {set_a | set_b}\")\n",
    "print(f\"Jaccard Similarity: {jaccard_similarity(set_a, set_b):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Worked Example: Comparing Two Documents\n",
    "\n",
    "Let's calculate the Jaccard similarity between two similar texts using k=2 shingles.\n",
    "\n",
    "This example is from the lecture slides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Documents from lecture example\n",
    "doc1 = \"I am Zexun\"\n",
    "doc2 = \"Zexun I am\"\n",
    "\n",
    "# Create shingles\n",
    "shingles1 = create_shingles(doc1, k=2)\n",
    "shingles2 = create_shingles(doc2, k=2)\n",
    "\n",
    "print(f\"Document 1: '{doc1}'\")\n",
    "print(f\"  Shingles: {shingles1}\")\n",
    "print()\n",
    "print(f\"Document 2: '{doc2}'\")\n",
    "print(f\"  Shingles: {shingles2}\")\n",
    "print()\n",
    "print(f\"Intersection: {shingles1 & shingles2}\")\n",
    "print(f\"Union: {shingles1 | shingles2}\")\n",
    "print()\n",
    "similarity = jaccard_similarity(shingles1, shingles2)\n",
    "print(f\"Jaccard Similarity: {similarity:.3f} ({similarity*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another example with more similar documents\n",
    "doc_a = \"The quick brown fox jumps over the lazy dog\"\n",
    "doc_b = \"The quick brown fox leaps over the lazy cat\"\n",
    "\n",
    "shingles_a = create_shingles(doc_a, k=2)\n",
    "shingles_b = create_shingles(doc_b, k=2)\n",
    "\n",
    "print(f\"Doc A shingles: {shingles_a}\")\n",
    "print(f\"Doc B shingles: {shingles_b}\")\n",
    "print()\n",
    "similarity = jaccard_similarity(shingles_a, shingles_b)\n",
    "print(f\"Jaccard Similarity: {similarity:.3f} ({similarity*100:.1f}%)\")\n",
    "print(f\"\\nAre they duplicates? {'Yes' if similarity > 0.8 else 'No'} (threshold: 80%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 5: PageRank and HITS Algorithms\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 The Random Surfer Model\n",
    "\n",
    "**Imagine**: You're bored and randomly clicking links on the web.\n",
    "\n",
    "- You start on some page\n",
    "- You click a random link to go to another page\n",
    "- You repeat this for hours...\n",
    "\n",
    "**Question**: Which pages would you visit most often?\n",
    "\n",
    "**Answer**: Pages that are linked to by many other pages (especially popular ones)!\n",
    "\n",
    "This is the core insight behind **PageRank** - the algorithm Google was founded on.\n",
    "\n",
    "### Markov Chains\n",
    "\n",
    "Mathematically, this random clicking is a **Markov Chain**:\n",
    "- **States** = web pages\n",
    "- **Transitions** = clicking links\n",
    "- **Key property**: Where you go next depends ONLY on where you are now (memoryless)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 PageRank Calculation\n",
    "\n",
    "**The Core Idea**: A page is important if important pages link to it.\n",
    "\n",
    "NetworkX makes PageRank calculation easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate PageRank for our demowebsite graph\n",
    "pageranks = nx.pagerank(graph)\n",
    "\n",
    "print(\"PageRank scores:\")\n",
    "print(\"-\" * 40)\n",
    "# Sort by PageRank value (highest first)\n",
    "sorted_pr = sorted(pageranks.items(), key=lambda x: x[1], reverse=True)\n",
    "for page, score in sorted_pr:\n",
    "    print(f\"{page:30} {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize with node sizes proportional to PageRank\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Calculate layout\n",
    "positions = nx.spring_layout(graph, seed=2026, k=2)\n",
    "\n",
    "# Node sizes based on PageRank (scaled up for visibility)\n",
    "sizes = [pageranks[node] * 10000 for node in graph.nodes()]\n",
    "\n",
    "# Draw\n",
    "nx.draw(graph, positions,\n",
    "        with_labels=True,\n",
    "        node_size=sizes,\n",
    "        node_color='lightcoral',\n",
    "        font_size=8,\n",
    "        arrows=True,\n",
    "        arrowsize=15)\n",
    "\n",
    "plt.title(\"DemoWebsite - Node Size = PageRank\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 The Dead End Problem and Teleportation\n",
    "\n",
    "**Problem**: What happens when the random surfer reaches a page with no outgoing links?\n",
    "\n",
    "They get **stuck**! This breaks our model.\n",
    "\n",
    "**Solution**: **Teleportation** (or \"damping factor\")\n",
    "\n",
    "With probability **alpha** (typically 0.15), the surfer:\n",
    "- Ignores the links\n",
    "- \"Teleports\" to a completely random page\n",
    "\n",
    "This is like opening a new browser tab and typing a random URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different alpha values\n",
    "print(\"Effect of damping factor (alpha):\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for alpha in [0.85, 0.50, 0.15]:\n",
    "    pr = nx.pagerank(graph, alpha=alpha)\n",
    "    # Get top 3 pages\n",
    "    top3 = sorted(pr.items(), key=lambda x: x[1], reverse=True)[:3]\n",
    "    print(f\"\\nalpha = {alpha}:\")\n",
    "    for page, score in top3:\n",
    "        print(f\"  {page}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 HITS Algorithm: Hubs vs Authorities\n",
    "\n",
    "**HITS** (Hyperlink-Induced Topic Search) recognizes two types of importance:\n",
    "\n",
    "1. **Authorities**: Pages that are linked to by many (experts on a topic)\n",
    "   - Example: Wikipedia article on \"Machine Learning\"\n",
    "\n",
    "2. **Hubs**: Pages that link to many good authorities (curators/directories)\n",
    "   - Example: \"Best ML Resources\" blog post with many links\n",
    "\n",
    "**Key insight**: Good hubs point to good authorities, and good authorities are pointed to by good hubs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate HITS scores\n",
    "hubs, authorities = nx.hits(graph)\n",
    "\n",
    "print(\"Hub Scores (pages that link to many):\")\n",
    "print(\"-\" * 40)\n",
    "sorted_hubs = sorted(hubs.items(), key=lambda x: x[1], reverse=True)\n",
    "for page, score in sorted_hubs:\n",
    "    print(f\"{page:30} {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Authority Scores (pages linked to by many):\")\n",
    "print(\"-\" * 40)\n",
    "sorted_auth = sorted(authorities.items(), key=lambda x: x[1], reverse=True)\n",
    "for page, score in sorted_auth:\n",
    "    print(f\"{page:30} {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 Comparison: PageRank vs HITS\n",
    "\n",
    "| Aspect | PageRank | HITS |\n",
    "|--------|----------|------|\n",
    "| **Scores** | Single importance score | Two scores (hub + authority) |\n",
    "| **Query-dependent** | No (computed once) | Yes (depends on search topic) |\n",
    "| **Best for** | Global importance ranking | Topic-specific searches |\n",
    "| **Dead end handling** | Teleportation | May have convergence issues |\n",
    "| **Used by** | Google (originally) | Older search engines |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison DataFrame\n",
    "comparison_data = []\n",
    "for page in graph.nodes():\n",
    "    comparison_data.append({\n",
    "        'Page': page,\n",
    "        'PageRank': round(pageranks[page], 4),\n",
    "        'Hub Score': round(hubs[page], 4),\n",
    "        'Authority': round(authorities[page], 4)\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(comparison_data)\n",
    "df = df.sort_values('PageRank', ascending=False)\n",
    "print(\"Comparison of All Ranking Scores:\")\n",
    "print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 6: LLM-Assisted Content Summarization (OPTIONAL)\n",
    "\n",
    "---\n",
    "\n",
    "**Note**: This section requires an OpenAI API key. If you don't have one, you can skip this part or just read through the code.\n",
    "\n",
    "Get your API key at: https://platform.openai.com/api-keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or University of Edinburgh has its own platform called ELM. ELM is the University of Edinburgh's AI innovation platform, a central gateway providing safer access to Generative Artificial Intelligence (GAI) via access to Large Language Models (LLMs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get your own free ELM API (limit to a fixed use) to use OpenAI model! Please refer to https://elm.edina.ac.uk/saml2/authenticate/elm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 When to Use LLMs for Web Data\n",
    "\n",
    "LLMs are useful when you need to:\n",
    "- Summarize large amounts of scraped text\n",
    "- Extract structured information from unstructured content\n",
    "- Categorize or classify web pages\n",
    "- Generate descriptions from product data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install OpenAI library if needed (uncomment to run)\n",
    "!pip install openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 OpenAI API Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "def summarize_with_openai(text, api_key):\n",
    "    \"\"\"\n",
    "    Generate a summary of text using OpenAI's API.\n",
    "    \n",
    "    Args:\n",
    "        text: The text to summarize\n",
    "        api_key: Your OpenAI API key\n",
    "        \n",
    "    Returns:\n",
    "        Summary string or None if error\n",
    "    \"\"\"\n",
    "    try:\n",
    "        client = OpenAI(api_key=api_key)\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant that summarizes web content concisely.\"},\n",
    "                {\"role\": \"user\", \"content\": f\"Please summarize the following text in 2-3 sentences:\\n\\n{text[:4000]}\"}\n",
    "            ],\n",
    "            max_tokens=150,\n",
    "            temperature=0.5\n",
    "        )\n",
    "        \n",
    "        return response.choices[0].message.content\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error with OpenAI API: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Scraping and Summarizing Web Pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_content(url):\n",
    "    \"\"\"\n",
    "    Scrape main content from a webpage.\n",
    "    \n",
    "    Args:\n",
    "        url: The URL to scrape\n",
    "        \n",
    "    Returns:\n",
    "        Text content or None if error\n",
    "    \"\"\"\n",
    "    try:\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "        }\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Remove unwanted elements\n",
    "        for tag in soup(['script', 'style', 'nav', 'footer', 'header']):\n",
    "            tag.decompose()\n",
    "        \n",
    "        # Extract main content\n",
    "        content = soup.find('main') or soup.find('article') or soup.find('body')\n",
    "        \n",
    "        if content:\n",
    "            return ' '.join(content.stripped_strings)\n",
    "        return None\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {url}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 Processing Multiple URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_urls_with_summaries(urls, api_key):\n",
    "    \"\"\"\n",
    "    Scrape multiple URLs and generate summaries.\n",
    "    \n",
    "    Args:\n",
    "        urls: List of URLs to process\n",
    "        api_key: OpenAI API key\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with URLs and summaries\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for url in urls:\n",
    "        print(f\"Processing: {url}\")\n",
    "        \n",
    "        # Scrape content\n",
    "        content = scrape_content(url)\n",
    "        \n",
    "        if content:\n",
    "            # Get summary\n",
    "            summary = summarize_with_openai(content, api_key)\n",
    "            \n",
    "            results.append({\n",
    "                'URL': url,\n",
    "                'Content Length': len(content),\n",
    "                'Summary': summary\n",
    "            })\n",
    "            \n",
    "            # Be polite - add delay\n",
    "            time.sleep(1)\n",
    "        else:\n",
    "            results.append({\n",
    "                'URL': url,\n",
    "                'Content Length': 0,\n",
    "                'Summary': 'Failed to scrape'\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage (uncomment and add your API key to run)\n",
    "\n",
    "# IMPORTANT: Replace with your actual API key\n",
    "# Never commit your API key to version control!\n",
    "OPENAI_API_KEY = \"YOUR_API_KEY_HERE\"\n",
    "\n",
    "# Better practice: Use environment variable\n",
    "# import os\n",
    "# OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY')\n",
    "\n",
    "urls_to_process = [\n",
    "    \"https://www.drps.ed.ac.uk/current/dpt/cxcmse11615.htm\",\n",
    "    \"https://www.drps.ed.ac.uk/current/dpt/cxcmse11427.htm\"\n",
    "]\n",
    "\n",
    "summary_table = process_urls_with_summaries(urls_to_process, OPENAI_API_KEY)\n",
    "print(summary_table)\n",
    "\n",
    "print(summary_table[\"Summary\"][0], \"\\n\",\n",
    "      summary_table[\"Summary\"][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Summary\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Web Crawling**: Automated link discovery using a frontier and visited set\n",
    "2. **Shingling**: Breaking text into k-word pieces for comparison\n",
    "3. **Jaccard Similarity**: Intersection/Union to measure document similarity\n",
    "4. **PageRank**: Random surfer model with teleportation for dead ends\n",
    "5. **HITS**: Hubs (link to many) vs Authorities (linked by many)\n",
    "\n",
    "## Best Practices\n",
    "\n",
    "- **Always check robots.txt** before crawling\n",
    "- **Add delays** between requests (`time.sleep(1)`)\n",
    "- **Limit crawl depth** for testing and politeness\n",
    "- **Handle errors gracefully** (try/except for network issues)\n",
    "- **Store results** to avoid re-crawling\n",
    "\n",
    "## Decision Tree: Which Algorithm?\n",
    "\n",
    "```\n",
    "Need to rank pages?\n",
    "    |\n",
    "    v\n",
    "Is ranking for a specific topic/query?\n",
    "    |\n",
    "  Yes     No\n",
    "    |      |\n",
    "    v      v\n",
    "  HITS   PageRank\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. Complete the **Week2-exercise.ipynb** to practice these concepts\n",
    "2. Try the **APC2 challenge** to apply crawling to real websites\n",
    "3. Explore the **NetworkX documentation** for more graph analysis options"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
