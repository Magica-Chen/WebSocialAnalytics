{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2: Exercise Solutions\n",
    "\n",
    "**Web and Social Network Analytics**\n",
    "\n",
    "---\n",
    "\n",
    "This notebook contains complete solutions for all exercises. **Try to solve them yourself first!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import os\n",
    "import time\n",
    "import pathlib\n",
    "from urllib.request import urlopen\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import pprint as pp\n",
    "\n",
    "# Web scraping\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# Graph analysis\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data handling\n",
    "import pandas as pd\n",
    "\n",
    "print('All libraries imported successfully!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for local files\n",
    "def create_local_file_address(folder, file):\n",
    "    \"\"\"Create a file:// URL that works on any operating system.\"\"\"\n",
    "    file_address = os.path.join(os.getcwd(), folder, file)\n",
    "    with_schema = pathlib.Path(file_address).as_uri()\n",
    "    return with_schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 1 Solution: Link Extraction Basics\n",
    "\n",
    "**Task**: Extract all hyperlinks from the `home.html` page in the `demowebsite` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution for Exercise 1\n",
    "# ========================\n",
    "\n",
    "# Step 1: Create the file path for home.html\n",
    "file_path = create_local_file_address(\"demowebsite\", \"home.html\")\n",
    "print(f\"Opening: {file_path}\")\n",
    "\n",
    "# Step 2: Open and parse the HTML file\n",
    "html = urlopen(file_path)\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# Step 3: Find all anchor (<a>) tags\n",
    "links = soup.find_all('a')\n",
    "print(f\"\\nFound {len(links)} anchor tags\")\n",
    "\n",
    "# Step 4: Extract and store href values in a list\n",
    "link_urls = []\n",
    "for link in links:\n",
    "    href = link.get('href')  # Safer than link['href'] - returns None if missing\n",
    "    if href:  # Only add if href exists\n",
    "        link_urls.append(href)\n",
    "        print(f\"  Link text: '{link.text}' -> {href}\")\n",
    "\n",
    "# Step 5: Print the results\n",
    "print(f\"\\nExtracted {len(link_urls)} links:\")\n",
    "print(link_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative Solution (using list comprehension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More concise version using list comprehension\n",
    "file_path = create_local_file_address(\"demowebsite\", \"home.html\")\n",
    "soup = BeautifulSoup(urlopen(file_path), 'html.parser')\n",
    "\n",
    "# One-liner to extract all hrefs\n",
    "link_urls = [link.get('href') for link in soup.find_all('a') if link.get('href')]\n",
    "\n",
    "print(f\"Found {len(link_urls)} links: {link_urls}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Mistakes\n",
    "\n",
    "1. **Using `link['href']` instead of `link.get('href')`**: The first will crash if href doesn't exist\n",
    "2. **Forgetting `file:///` protocol**: Local files need the file:// scheme\n",
    "3. **Getting link text instead of href**: `link.text` gives the visible text, not the URL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 2 Solution: Shingling and Jaccard Similarity\n",
    "\n",
    "**Task**: Calculate Jaccard similarity between two documents using k=2 word shingles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution for Exercise 2\n",
    "# ========================\n",
    "\n",
    "# Step 1: Define the documents\n",
    "doc_a = \"The quick brown fox jumps\"\n",
    "doc_b = \"The quick red fox leaps\"\n",
    "\n",
    "# Step 2: Implement the create_shingles function\n",
    "def create_shingles(text, k=2):\n",
    "    \"\"\"\n",
    "    Create k-word shingles from text.\n",
    "    \n",
    "    Args:\n",
    "        text: Input string\n",
    "        k: Number of words per shingle\n",
    "    \n",
    "    Returns:\n",
    "        Set of shingles\n",
    "    \"\"\"\n",
    "    # Normalize: lowercase and split into words\n",
    "    words = text.lower().split()\n",
    "    \n",
    "    # Create overlapping shingles\n",
    "    shingles = set()\n",
    "    for i in range(len(words) - k + 1):\n",
    "        # Join k consecutive words\n",
    "        shingle = ' '.join(words[i:i+k])\n",
    "        shingles.add(shingle)\n",
    "    \n",
    "    return shingles\n",
    "\n",
    "# Step 3: Create shingles for both documents\n",
    "shingles_a = create_shingles(doc_a, k=2)\n",
    "shingles_b = create_shingles(doc_b, k=2)\n",
    "\n",
    "# Step 4: Implement Jaccard similarity function\n",
    "def jaccard_similarity(set1, set2):\n",
    "    \"\"\"\n",
    "    Calculate Jaccard similarity between two sets.\n",
    "    \n",
    "    Jaccard = |Intersection| / |Union|\n",
    "    \"\"\"\n",
    "    intersection = set1 & set2  # Elements in both sets\n",
    "    union = set1 | set2         # Elements in either set\n",
    "    \n",
    "    if len(union) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    return len(intersection) / len(union)\n",
    "\n",
    "# Step 5: Calculate and display results\n",
    "print(f\"Document A: '{doc_a}'\")\n",
    "print(f\"Shingles A: {shingles_a}\")\n",
    "print()\n",
    "print(f\"Document B: '{doc_b}'\")\n",
    "print(f\"Shingles B: {shingles_b}\")\n",
    "print()\n",
    "\n",
    "intersection = shingles_a & shingles_b\n",
    "union = shingles_a | shingles_b\n",
    "\n",
    "print(f\"Intersection: {intersection}\")\n",
    "print(f\"  Count: {len(intersection)}\")\n",
    "print()\n",
    "print(f\"Union: {union}\")\n",
    "print(f\"  Count: {len(union)}\")\n",
    "print()\n",
    "\n",
    "similarity = jaccard_similarity(shingles_a, shingles_b)\n",
    "print(f\"Jaccard Similarity: {len(intersection)}/{len(union)} = {similarity:.4f} ({similarity*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus: Lecture Example (\"I am Zexun\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example from the lecture slides\n",
    "doc1 = \"I am Zexun\"\n",
    "doc2 = \"Zexun I am\"\n",
    "\n",
    "s1 = create_shingles(doc1, k=2)\n",
    "s2 = create_shingles(doc2, k=2)\n",
    "\n",
    "print(f\"'{doc1}' shingles: {s1}\")\n",
    "print(f\"'{doc2}' shingles: {s2}\")\n",
    "print(f\"\\nJaccard Similarity: {jaccard_similarity(s1, s2):.4f}\")\n",
    "print(f\"Expected: 1/3 = 0.3333\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 3 Solution: Building a Complete Web Crawler\n",
    "\n",
    "**Task**: Implement a crawler that maps all pages in the `demowebsite` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution for Exercise 3\n",
    "# ========================\n",
    "\n",
    "def visit_page_and_return_dictionary(page_url):\n",
    "    \"\"\"\n",
    "    Visit a local HTML page and return structured link data.\n",
    "    \n",
    "    Args:\n",
    "        page_url: Name of the HTML file (e.g., 'home.html')\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with 'address' and 'links_to' keys\n",
    "    \"\"\"\n",
    "    # Create full file path\n",
    "    full_path = create_local_file_address(\"demowebsite\", page_url)\n",
    "    print(f\"Visiting: {page_url}\")\n",
    "    \n",
    "    try:\n",
    "        # Open and parse the page\n",
    "        html = urlopen(full_path)\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        \n",
    "        # Find all links\n",
    "        links = soup.find_all('a')\n",
    "        link_urls = []\n",
    "        \n",
    "        for link in links:\n",
    "            href = link.get('href')\n",
    "            if href:\n",
    "                link_urls.append(href)\n",
    "        \n",
    "        return {\n",
    "            'address': page_url,\n",
    "            'links_to': link_urls\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"  Error: {e}\")\n",
    "        return {\n",
    "            'address': page_url,\n",
    "            'links_to': []\n",
    "        }\n",
    "\n",
    "# Initialize data structures\n",
    "starting_website = \"home.html\"\n",
    "pages_visited = []              # Track what we've processed\n",
    "pages_to_visit = [starting_website]  # The frontier\n",
    "pages_info = []                 # Store results\n",
    "\n",
    "# Main crawl loop\n",
    "while len(pages_to_visit) > 0:\n",
    "    # Get next page from frontier\n",
    "    current_page = pages_to_visit.pop()\n",
    "    \n",
    "    # Skip if already visited\n",
    "    if current_page in pages_visited:\n",
    "        continue\n",
    "    \n",
    "    # Visit and get link info\n",
    "    page_data = visit_page_and_return_dictionary(current_page)\n",
    "    pages_info.append(page_data)\n",
    "    \n",
    "    # Mark as visited\n",
    "    pages_visited.append(current_page)\n",
    "    \n",
    "    # Add new links to frontier\n",
    "    for link in page_data['links_to']:\n",
    "        if link not in pages_visited and link not in pages_to_visit:\n",
    "            pages_to_visit.append(link)\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"CRAWLING COMPLETE!\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nCrawled {len(pages_visited)} pages: {pages_visited}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View complete link structure\n",
    "print(\"Complete link structure:\")\n",
    "print(\"-\" * 50)\n",
    "pp.pprint(pages_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Mistakes\n",
    "\n",
    "1. **Not checking if already visited**: Can cause infinite loops\n",
    "2. **Adding duplicates to frontier**: Check both visited AND to_visit lists\n",
    "3. **Not handling errors**: Pages might not exist or have issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 4 Solution: PageRank Calculation\n",
    "\n",
    "**Task**: Build a NetworkX graph from crawler data and calculate PageRank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution for Exercise 4\n",
    "# ========================\n",
    "\n",
    "# Step 1: Create a directed graph from crawler data\n",
    "graph = nx.DiGraph()\n",
    "\n",
    "for page in pages_info:\n",
    "    origin = page['address']\n",
    "    for destination in page['links_to']:\n",
    "        graph.add_edge(origin, destination)\n",
    "\n",
    "print(f\"Graph created with {graph.number_of_nodes()} nodes and {graph.number_of_edges()} edges\")\n",
    "\n",
    "# Step 2: Calculate PageRank\n",
    "pageranks = nx.pagerank(graph, alpha=0.85)  # alpha is damping factor\n",
    "\n",
    "# Step 3: Print sorted PageRank scores\n",
    "print(\"\\nPageRank Scores (highest to lowest):\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "sorted_pr = sorted(pageranks.items(), key=lambda x: x[1], reverse=True)\n",
    "for page, score in sorted_pr:\n",
    "    print(f\"{page:35} {score:.4f}\")\n",
    "\n",
    "# Identify highest and lowest\n",
    "highest = sorted_pr[0]\n",
    "lowest = sorted_pr[-1]\n",
    "\n",
    "print(\"\\n\" + \"-\" * 45)\n",
    "print(f\"Highest PageRank: {highest[0]} ({highest[1]:.4f})\")\n",
    "print(f\"Lowest PageRank:  {lowest[0]} ({lowest[1]:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Visualize the graph with node sizes based on PageRank\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "# Calculate layout\n",
    "positions = nx.spring_layout(graph, seed=42, k=2)\n",
    "\n",
    "# Node sizes based on PageRank (scaled up for visibility)\n",
    "sizes = [pageranks[node] * 8000 for node in graph.nodes()]\n",
    "\n",
    "# Node colors based on PageRank (darker = higher)\n",
    "colors = [pageranks[node] for node in graph.nodes()]\n",
    "\n",
    "# Draw nodes\n",
    "nx.draw_networkx_nodes(graph, positions, \n",
    "                       node_size=sizes, \n",
    "                       node_color=colors, \n",
    "                       cmap=plt.cm.Reds,\n",
    "                       alpha=0.8)\n",
    "\n",
    "# Draw edges\n",
    "nx.draw_networkx_edges(graph, positions, \n",
    "                       edge_color='gray', \n",
    "                       arrows=True, \n",
    "                       arrowsize=15,\n",
    "                       alpha=0.6)\n",
    "\n",
    "# Draw labels\n",
    "nx.draw_networkx_labels(graph, positions, font_size=8)\n",
    "\n",
    "plt.title(\"DemoWebsite - Node Size & Color = PageRank\", fontsize=14)\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "**Which page has the highest PageRank? Why?**\n",
    "\n",
    "The page with the highest PageRank is typically `home.html` or `shop.html` because:\n",
    "1. It's linked to by multiple other pages (many incoming links)\n",
    "2. The pages linking to it also have decent PageRank (quality of links matters)\n",
    "3. In this demo website, `home.html` serves as the central hub\n",
    "\n",
    "Pages with low PageRank (like `team.html`) typically have:\n",
    "- Few incoming links\n",
    "- Are \"dead ends\" with no outgoing links (though this affects others' PageRank more than their own)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 5 Solution: HITS Algorithm Comparison\n",
    "\n",
    "**Task**: Calculate HITS scores and compare with PageRank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution for Exercise 5\n",
    "# ========================\n",
    "\n",
    "# Step 1: Calculate HITS scores\n",
    "hubs, authorities = nx.hits(graph, max_iter=100)\n",
    "\n",
    "# Step 2: Print hub scores (sorted)\n",
    "print(\"Hub Scores (pages that link to many good authorities):\")\n",
    "print(\"-\" * 50)\n",
    "sorted_hubs = sorted(hubs.items(), key=lambda x: x[1], reverse=True)\n",
    "for page, score in sorted_hubs:\n",
    "    print(f\"{page:35} {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Print authority scores (sorted)\n",
    "print(\"Authority Scores (pages linked to by many good hubs):\")\n",
    "print(\"-\" * 50)\n",
    "sorted_auth = sorted(authorities.items(), key=lambda x: x[1], reverse=True)\n",
    "for page, score in sorted_auth:\n",
    "    print(f\"{page:35} {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Create comparison DataFrame\n",
    "comparison_data = []\n",
    "for page in graph.nodes():\n",
    "    comparison_data.append({\n",
    "        'Page': page,\n",
    "        'PageRank': round(pageranks[page], 4),\n",
    "        'Hub Score': round(hubs[page], 4),\n",
    "        'Authority': round(authorities[page], 4),\n",
    "        'In-Degree': graph.in_degree(page),\n",
    "        'Out-Degree': graph.out_degree(page)\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(comparison_data)\n",
    "df = df.sort_values('PageRank', ascending=False)\n",
    "\n",
    "print(\"Comparison Table:\")\n",
    "print(\"=\" * 80)\n",
    "print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify best hub and authority\n",
    "best_hub = max(hubs.items(), key=lambda x: x[1])\n",
    "best_authority = max(authorities.items(), key=lambda x: x[1])\n",
    "\n",
    "print(f\"\\nBest Hub: {best_hub[0]} (score: {best_hub[1]:.4f})\")\n",
    "print(f\"  - Out-degree: {graph.out_degree(best_hub[0])} outgoing links\")\n",
    "\n",
    "print(f\"\\nBest Authority: {best_authority[0]} (score: {best_authority[1]:.4f})\")\n",
    "print(f\"  - In-degree: {graph.in_degree(best_authority[0])} incoming links\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "**1. Which page is the best hub? Why?**\n",
    "\n",
    "The best **hub** is typically `home.html` because:\n",
    "- It has the most outgoing links (highest out-degree)\n",
    "- A hub's score depends on the authority scores of pages it links to\n",
    "- Good hubs are \"directories\" that point to many valuable resources\n",
    "\n",
    "**2. Which page is the best authority? Why?**\n",
    "\n",
    "The best **authority** is often `shop.html` or `home.html` because:\n",
    "- It's linked to by many other pages (high in-degree)\n",
    "- An authority's score depends on the hub scores of pages linking to it\n",
    "- Good authorities are \"experts\" that everyone references\n",
    "\n",
    "**3. Why might HITS rankings differ from PageRank?**\n",
    "\n",
    "- **PageRank** assigns ONE score per page based on incoming links and their importance\n",
    "- **HITS** assigns TWO scores (hub + authority) recognizing different types of importance\n",
    "- A page can be a great hub (links out a lot) but poor authority (not linked to much)\n",
    "- PageRank includes teleportation; HITS doesn't handle dead ends the same way\n",
    "- HITS is typically query-dependent in practice (computed for search results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Bonus Challenge Solution: Remote Website Crawler\n",
    "\n",
    "**Task**: Adapt the crawler to work with real websites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bonus Solution: Remote Website Crawler\n",
    "# =======================================\n",
    "\n",
    "def get_domain(url):\n",
    "    \"\"\"Extract domain from URL.\"\"\"\n",
    "    parsed = urlparse(url)\n",
    "    return parsed.netloc\n",
    "\n",
    "def clean_url(base_url, link):\n",
    "    \"\"\"\n",
    "    Convert relative URLs to absolute and clean up.\n",
    "    \n",
    "    Args:\n",
    "        base_url: The page URL where the link was found\n",
    "        link: The href value (may be relative or absolute)\n",
    "    \n",
    "    Returns:\n",
    "        Clean absolute URL or None if invalid\n",
    "    \"\"\"\n",
    "    # Skip non-page links\n",
    "    if not link or link.startswith('#') or link.startswith('mailto:') or link.startswith('javascript:'):\n",
    "        return None\n",
    "    \n",
    "    # Convert relative to absolute\n",
    "    full_url = urljoin(base_url, link)\n",
    "    \n",
    "    # Remove fragments and query strings for cleaner comparison\n",
    "    parsed = urlparse(full_url)\n",
    "    clean = f\"{parsed.scheme}://{parsed.netloc}{parsed.path}\"\n",
    "    \n",
    "    # Remove trailing slash for consistency\n",
    "    clean = clean.rstrip('/')\n",
    "    \n",
    "    return clean\n",
    "\n",
    "def visit_remote_page(url, target_domain):\n",
    "    \"\"\"\n",
    "    Visit a remote URL and return links (filtered to same domain).\n",
    "    \n",
    "    Args:\n",
    "        url: URL to visit\n",
    "        target_domain: Only return links within this domain\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with 'address' and 'links_to'\n",
    "    \"\"\"\n",
    "    print(f\"Visiting: {url}\")\n",
    "    \n",
    "    try:\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Educational Web Crawler)'\n",
    "        }\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Find all links\n",
    "        links = soup.find_all('a')\n",
    "        valid_links = []\n",
    "        \n",
    "        for link in links:\n",
    "            href = link.get('href')\n",
    "            clean = clean_url(url, href)\n",
    "            \n",
    "            if clean and get_domain(clean) == target_domain:\n",
    "                if clean not in valid_links:  # Avoid duplicates\n",
    "                    valid_links.append(clean)\n",
    "        \n",
    "        return {\n",
    "            'address': url,\n",
    "            'links_to': valid_links\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"  Error: {e}\")\n",
    "        return {\n",
    "            'address': url,\n",
    "            'links_to': []\n",
    "        }\n",
    "\n",
    "def crawl_website(start_url, max_pages=10):\n",
    "    \"\"\"\n",
    "    Crawl a website starting from start_url.\n",
    "    \n",
    "    Args:\n",
    "        start_url: URL to start crawling from\n",
    "        max_pages: Maximum number of pages to crawl\n",
    "    \n",
    "    Returns:\n",
    "        List of page info dictionaries\n",
    "    \"\"\"\n",
    "    target_domain = get_domain(start_url)\n",
    "    print(f\"Crawling domain: {target_domain}\")\n",
    "    print(f\"Max pages: {max_pages}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Clean start URL\n",
    "    start_url = clean_url(start_url, start_url)\n",
    "    \n",
    "    pages_visited = []\n",
    "    pages_to_visit = [start_url]\n",
    "    pages_info = []\n",
    "    \n",
    "    while len(pages_to_visit) > 0 and len(pages_visited) < max_pages:\n",
    "        current = pages_to_visit.pop(0)  # FIFO for breadth-first\n",
    "        \n",
    "        if current in pages_visited:\n",
    "            continue\n",
    "        \n",
    "        page_data = visit_remote_page(current, target_domain)\n",
    "        pages_info.append(page_data)\n",
    "        pages_visited.append(current)\n",
    "        \n",
    "        # Add new links to frontier\n",
    "        for link in page_data['links_to']:\n",
    "            if link not in pages_visited and link not in pages_to_visit:\n",
    "                pages_to_visit.append(link)\n",
    "        \n",
    "        # Be polite!\n",
    "        time.sleep(1)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(f\"Crawled {len(pages_visited)} pages\")\n",
    "    \n",
    "    return pages_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with quotes.toscrape.com (a site designed for scraping practice)\n",
    "results = crawl_website(\"https://quotes.toscrape.com\", max_pages=5)\n",
    "\n",
    "print(\"\\nCrawl Results:\")\n",
    "for page in results:\n",
    "    print(f\"\\n{page['address']}\")\n",
    "    print(f\"  Links to {len(page['links_to'])} pages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and visualize the graph\n",
    "if results:\n",
    "    remote_graph = nx.DiGraph()\n",
    "    \n",
    "    for page in results:\n",
    "        for dest in page['links_to']:\n",
    "            # Shorten URLs for display\n",
    "            origin_short = page['address'].replace('https://quotes.toscrape.com', '')\n",
    "            dest_short = dest.replace('https://quotes.toscrape.com', '')\n",
    "            if not origin_short: origin_short = '/'\n",
    "            if not dest_short: dest_short = '/'\n",
    "            remote_graph.add_edge(origin_short, dest_short)\n",
    "    \n",
    "    # Calculate PageRank\n",
    "    if remote_graph.number_of_nodes() > 0:\n",
    "        pr = nx.pagerank(remote_graph)\n",
    "        \n",
    "        plt.figure(figsize=(12, 8))\n",
    "        pos = nx.spring_layout(remote_graph, seed=42)\n",
    "        sizes = [pr[n] * 5000 for n in remote_graph.nodes()]\n",
    "        \n",
    "        nx.draw(remote_graph, pos, with_labels=True, node_size=sizes,\n",
    "                node_color='lightgreen', font_size=8, arrows=True)\n",
    "        plt.title(\"quotes.toscrape.com Link Structure\")\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"\\nPageRank Scores:\")\n",
    "        for page, score in sorted(pr.items(), key=lambda x: x[1], reverse=True)[:5]:\n",
    "            print(f\"  {page}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "In these exercises, you practiced:\n",
    "\n",
    "1. **Link Extraction**: Using BeautifulSoup to parse HTML and extract hyperlinks\n",
    "2. **Shingling**: Breaking text into overlapping pieces for comparison\n",
    "3. **Jaccard Similarity**: Measuring document similarity using set operations\n",
    "4. **Web Crawling**: Managing a frontier and avoiding revisits\n",
    "5. **PageRank**: Measuring page importance based on link structure\n",
    "6. **HITS**: Understanding hubs vs authorities\n",
    "\n",
    "These are fundamental concepts for search engines and web analytics!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
