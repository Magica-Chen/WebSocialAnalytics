{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1: Introduction to Web Scraping\n",
    "\n",
    "**Web and Social Network Analytics**\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this lab, you will be able to:\n",
    "\n",
    "1. Understand when and why to scrape web data\n",
    "2. Use `requests` + `BeautifulSoup` for static HTML pages\n",
    "3. Use `Playwright` for JavaScript-rendered content\n",
    "4. Use APIs as the preferred data source\n",
    "\n",
    "---\n",
    "\n",
    "**Disclaimer**: This educational content, including any code examples, is provided for instructional purposes only. The author does not endorse or encourage the unauthorised or illegal scraping of websites.\n",
    "\n",
    "While Python with relevant libraries can be used for web scraping, it's crucial to conduct scraping activities in compliance with applicable laws, the website's terms of service, and ethical considerations. Always review and respect the rules set by the website you are scraping to ensure legal and responsible data collection practices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: Introduction to Web Scraping\n",
    "\n",
    "## What is Web Scraping?\n",
    "\n",
    "**Web scraping** is the automated process of extracting data from websites. Instead of manually copying information, we write programs that:\n",
    "\n",
    "1. Download web pages\n",
    "2. Parse the HTML content\n",
    "3. Extract the data we need\n",
    "4. Store it in a structured format (CSV, database, etc.)\n",
    "\n",
    "### Common Use Cases\n",
    "\n",
    "- **Price monitoring**: Track product prices across e-commerce sites\n",
    "- **Research**: Collect data for academic studies\n",
    "- **News aggregation**: Gather articles from multiple sources\n",
    "- **Social media analysis**: Analyze public posts and trends\n",
    "- **Job listings**: Aggregate job postings from various sites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Web Scraping Decision Tree\n",
    "\n",
    "Before scraping, always follow this decision process:\n",
    "\n",
    "```\n",
    "Do you need data from a website?\n",
    "            |\n",
    "            v\n",
    "    1. Does an official API exist?\n",
    "            |\n",
    "       Yes  |  No\n",
    "        |   |   |\n",
    "        v   |   v\n",
    "    Use the |  2. Is the content static HTML?\n",
    "      API!  |       |\n",
    "            |  Yes  |  No (JavaScript-rendered)\n",
    "            |   |   |   |\n",
    "            |   v   |   v\n",
    "            | Use   | Use Playwright\n",
    "            | BeautifulSoup  or Selenium\n",
    "```\n",
    "\n",
    "**Always prefer APIs** when available - they provide:\n",
    "- Structured data (JSON/XML)\n",
    "- Legal access with terms of service\n",
    "- Reliable and stable endpoints\n",
    "- Rate limiting to prevent overload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HTTP Basics\n",
    "\n",
    "The web works on the **HTTP protocol**. When you visit a website:\n",
    "\n",
    "1. Your browser sends an **HTTP GET request** to the server\n",
    "2. The server processes the request\n",
    "3. The server sends back an **HTTP response** with:\n",
    "   - **Status code** (200 = OK, 404 = Not Found, 500 = Server Error)\n",
    "   - **Headers** (metadata about the response)\n",
    "   - **Body** (the actual HTML content)\n",
    "\n",
    "### Common Status Codes (This is common, NOT MUST BE)\n",
    "\n",
    "| Code | Meaning |\n",
    "|------|---------|  \n",
    "| 200 | OK - Request successful |\n",
    "| 301 | Moved Permanently - Redirect |\n",
    "| 403 | Forbidden - Access denied |\n",
    "| 404 | Not Found - Page doesn't exist |\n",
    "| 500 | Internal Server Error |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: HTML Fundamentals\n",
    "\n",
    "## HTML Structure\n",
    "\n",
    "HTML (HyperText Markup Language) structures web content using **tags**:\n",
    "\n",
    "```html\n",
    "<tagname attribute=\"value\">Content</tagname>\n",
    "```\n",
    "\n",
    "### Key HTML Elements\n",
    "\n",
    "| Tag | Purpose | Example |\n",
    "|-----|---------|---------|  \n",
    "| `<h1>` to `<h6>` | Headings | `<h1>Main Title</h1>` |\n",
    "| `<p>` | Paragraph | `<p>Some text...</p>` |\n",
    "| `<a>` | Hyperlink | `<a href=\"url\">Link text</a>` |\n",
    "| `<div>` | Division/container | `<div class=\"section\">...</div>` |\n",
    "| `<span>` | Inline container | `<span class=\"highlight\">text</span>` |\n",
    "| `<table>` | Table | Contains `<tr>`, `<td>` |\n",
    "| `<ul>`, `<ol>` | Lists | Contains `<li>` items |\n",
    "\n",
    "### Finding Elements\n",
    "\n",
    "Elements can be identified by:\n",
    "\n",
    "1. **Tag name**: `<div>`, `<p>`, `<a>`\n",
    "2. **ID** (unique): `<div id=\"header\">` - only one element per page\n",
    "3. **Class** (reusable): `<div class=\"card\">` - multiple elements can share"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hands-on: Exploring HTML\n",
    "\n",
    "Let's explore the `example_html.html` file in this folder. Open it in your browser and use **Developer Tools** (F12 or right-click > Inspect) to examine the structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's import the libraries we'll need\n",
    "import os\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the local HTML file\n",
    "file_url = \"file:///\" + os.getcwd() + \"/example_html.html\"\n",
    "website_source_code = urlopen(file_url)\n",
    "\n",
    "# Parse with BeautifulSoup\n",
    "soup = BeautifulSoup(website_source_code, 'html.parser')\n",
    "\n",
    "# View the formatted HTML\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 3: Static Scraping with BeautifulSoup\n",
    "\n",
    "**BeautifulSoup** is a Python library for parsing HTML and XML documents. It creates a parse tree that makes it easy to extract data.\n",
    "\n",
    "## Core Pattern\n",
    "\n",
    "```python\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 1. Fetch the page\n",
    "html = urlopen(url)\n",
    "\n",
    "# 2. Parse with BeautifulSoup\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# 3. Find elements\n",
    "elements = soup.find_all('tag', {'class': 'classname'})\n",
    "\n",
    "# 4. Extract data\n",
    "for element in elements:\n",
    "    print(element.text)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key BeautifulSoup Methods\n",
    "\n",
    "| Method | Returns | Example |\n",
    "|--------|---------|---------|  \n",
    "| `find(tag)` | First matching element | `soup.find('h1')` |\n",
    "| `find_all(tag)` | List of all matches | `soup.find_all('p')` |\n",
    "| `find(id='x')` | Element with ID | `soup.find(id='header')` |\n",
    "| `find(class_='x')` | Element with class | `soup.find(class_='card')` |\n",
    "| `find('tag', {'attr': 'val'})` | By attribute | `soup.find('div', {'class': 'main'})` |\n",
    "| `.text` | Text content | `element.text` |\n",
    "| `['attribute']` | Attribute value | `link['href']` |\n",
    "| `.findChildren()` | Child elements | `row.findChildren('td')` |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Working with Local HTML\n",
    "\n",
    "Let's practice finding elements in our local HTML file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all h1 tags\n",
    "h1_tags = soup.find_all('h1')\n",
    "\n",
    "for h1 in h1_tags:\n",
    "    print('Tag:', h1)\n",
    "    print('Text:', h1.text)\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find element by ID\n",
    "middle_row = soup.find(id='middle_row')\n",
    "\n",
    "print('Complete tag:', middle_row)\n",
    "print('Text content:', middle_row.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find children of an element\n",
    "cells = middle_row.findChildren('td')\n",
    "\n",
    "for cell in cells:\n",
    "    print('Cell value:', cell.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find by class name\n",
    "hipster_divs = soup.find_all('div', {'class': 'hipster'})\n",
    "\n",
    "for div in hipster_divs:\n",
    "    # Get the h2 inside each div\n",
    "    header = div.find('h2').text\n",
    "    paragraph = div.find('p').text.strip()\n",
    "    print(f'Header: {header}')\n",
    "    print(f'Content: {paragraph}')\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all data from a table\n",
    "table = soup.find('table')\n",
    "\n",
    "for row_num, row in enumerate(table.find_all('tr')):\n",
    "    print(f'Row {row_num}:')\n",
    "    for cell in row.find_all('td'):\n",
    "        print(f'  Value: {cell.text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Scraping quotes.toscrape.com\n",
    "\n",
    "This is a website specifically designed for practicing web scraping. It's stable and won't change unexpectedly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the quotes page\n",
    "url = 'https://quotes.toscrape.com/'\n",
    "html = urlopen(url)\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# Find all quote containers\n",
    "quotes = soup.find_all('div', {'class': 'quote'})\n",
    "\n",
    "print(f'Found {len(quotes)} quotes on this page\\n')\n",
    "\n",
    "# Extract data from each quote\n",
    "for quote in quotes[:5]:  # First 5 quotes\n",
    "    text = quote.find('span', {'class': 'text'}).text\n",
    "    author = quote.find('small', {'class': 'author'}).text\n",
    "    \n",
    "    # Get tags\n",
    "    tags = [tag.text for tag in quote.find_all('a', {'class': 'tag'})]\n",
    "    \n",
    "    print(f'Quote: {text}')\n",
    "    print(f'Author: {author}')\n",
    "    print(f'Tags: {tags}')\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Pagination\n",
    "\n",
    "Most websites split content across multiple pages. Let's scrape multiple pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "all_quotes = []\n",
    "\n",
    "# Scrape first 3 pages\n",
    "for page_num in range(1, 4):\n",
    "    url = f'https://quotes.toscrape.com/page/{page_num}/'\n",
    "    print(f'Scraping page {page_num}...')\n",
    "    \n",
    "    html = urlopen(url)\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    quotes = soup.find_all('div', {'class': 'quote'})\n",
    "    \n",
    "    for quote in quotes:\n",
    "        all_quotes.append({\n",
    "            'text': quote.find('span', {'class': 'text'}).text,\n",
    "            'author': quote.find('small', {'class': 'author'}).text,\n",
    "            'tags': [tag.text for tag in quote.find_all('a', {'class': 'tag'})]\n",
    "        })\n",
    "    \n",
    "    # Be respectful - wait between requests\n",
    "    time.sleep(1)\n",
    "\n",
    "print(f'\\nTotal quotes collected: {len(all_quotes)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to a pandas DataFrame\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(all_quotes)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Edinburgh University DRPS\n",
    "\n",
    "Let's scrape real course information from the University of Edinburgh's DRPS (Degree Regulations and Programmes of Study)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch a course page\n",
    "url = 'https://www.drps.ed.ac.uk/current/dpt/cxcmse11427.htm'\n",
    "html = urlopen(url)\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# The page title contains the course name\n",
    "course_name = soup.find('h1').text if soup.find('h1') else 'Not found'\n",
    "print(f'Course: {course_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find information in the course table\n",
    "table = soup.find('table', {'class': 'sitstablegrid'})\n",
    "\n",
    "if table:\n",
    "    for cell in table.find_all('td'):\n",
    "        text = cell.text.strip()\n",
    "        # Look for specific information\n",
    "        if 'SCQF' in text:\n",
    "            print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try It Yourself!\n",
    "\n",
    "Modify the code above to extract:\n",
    "1. The course credits\n",
    "2. The course organiser\n",
    "3. The course description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 4: Dynamic Scraping with Playwright\n",
    "\n",
    "## Why Do We Need Playwright?\n",
    "\n",
    "Many modern websites use **JavaScript** to load content dynamically. When you visit such a site:\n",
    "\n",
    "1. The server sends minimal HTML\n",
    "2. JavaScript code runs in your browser\n",
    "3. The JavaScript fetches data and renders the content\n",
    "\n",
    "**Problem**: BeautifulSoup only sees the initial HTML - not the JavaScript-rendered content!\n",
    "\n",
    "**Solution**: Use a browser automation tool like **Playwright** that:\n",
    "- Launches a real browser\n",
    "- Executes JavaScript\n",
    "- Waits for content to load\n",
    "- Then extracts the rendered HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Setting Up Playwright\n\nPlaywright works well in JupyterHub environments because:\n- It manages browser binaries automatically\n- It has excellent headless mode support\n- It's designed for modern web applications\n\n### Async vs Sync API: Which to Use?\n\n| Environment | API to Use | Reason |\n|-------------|------------|--------|\n| **JupyterLab/Notebook** | `async_playwright` | JupyterLab already runs an event loop; async avoids conflicts |\n| **.py scripts** | `sync_playwright` | Simpler syntax when no event loop is running |\n\n**In this notebook**, we use the **async API** since you're running JupyterLab. The sync API equivalent is shown in comments for reference when writing standalone scripts."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Playwright (run once)\n",
    "# !pip install playwright\n",
    "\n",
    "# Install browser binaries (run once)\n",
    "# !playwright install chromium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Import Playwright\n# For JupyterLab/Notebook: Use async API (recommended)\nfrom playwright.async_api import async_playwright\n\n# For .py scripts: Use sync API (uncomment below)\n# from playwright.sync_api import sync_playwright"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Core Pattern\n\n### For JupyterLab/Notebook: Async API (Recommended)\n\nJupyterLab runs an event loop, so using the async API avoids conflicts and is the recommended approach.\n\n```python\nfrom playwright.async_api import async_playwright\n\nasync def scrape_page():\n    async with async_playwright() as p:\n        # Launch browser in headless mode (no GUI)\n        browser = await p.chromium.launch(headless=True)\n        page = await browser.new_page()\n        \n        # Navigate to URL\n        await page.goto('https://example.com')\n        \n        # Wait for content to load\n        await page.wait_for_selector('.content-class')\n        \n        # Get the rendered HTML\n        html = await page.content()\n        \n        # Now use BeautifulSoup to parse\n        soup = BeautifulSoup(html, 'html.parser')\n        \n        # Close browser\n        await browser.close()\n        \n        return soup\n\n# Run in JupyterLab/Notebook\nsoup = await scrape_page()\n```\n\n### For .py Scripts: Sync API\n\nWhen running standalone Python scripts (.py files), use the sync API:\n\n```python\nfrom playwright.sync_api import sync_playwright\n\nwith sync_playwright() as p:\n    # Launch browser in headless mode (no GUI)\n    browser = p.chromium.launch(headless=True)\n    page = browser.new_page()\n    \n    # Navigate to URL\n    page.goto('https://example.com')\n    \n    # Wait for content to load\n    page.wait_for_selector('.content-class')\n    \n    # Get the rendered HTML\n    html = page.content()\n    \n    # Now use BeautifulSoup to parse\n    soup = BeautifulSoup(html, 'html.parser')\n    \n    # Close browser\n    browser.close()\n```"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Playwright Methods\n",
    "\n",
    "| Method | Purpose | Example |\n",
    "|--------|---------|---------|  \n",
    "| `page.goto(url)` | Navigate to URL | `page.goto('https://...')` |\n",
    "| `page.wait_for_selector(sel)` | Wait for element | `page.wait_for_selector('.quote')` |\n",
    "| `page.click(sel)` | Click element | `page.click('button.next')` |\n",
    "| `page.fill(sel, text)` | Fill input field | `page.fill('#search', 'query')` |\n",
    "| `page.content()` | Get HTML content | `html = page.content()` |\n",
    "| `page.screenshot()` | Take screenshot | `page.screenshot(path='shot.png')` |\n",
    "| `page.evaluate(js)` | Run JavaScript | `page.evaluate('window.scrollBy(0, 500)')` |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: JavaScript-Rendered Quotes\n",
    "\n",
    "The website `quotes.toscrape.com/js/` renders quotes using JavaScript. BeautifulSoup alone cannot see them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's see what BeautifulSoup gets (without JavaScript)\n",
    "url = 'https://quotes.toscrape.com/js/'\n",
    "html = urlopen(url)\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "quotes = soup.find_all('div', {'class': 'quote'})\n",
    "print(f'BeautifulSoup found: {len(quotes)} quotes')\n",
    "print('(The quotes are loaded by JavaScript, so BeautifulSoup sees nothing!)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Now let's use Playwright to render the JavaScript\n# ============================================\n# ASYNC API (for JupyterLab/Notebook)\n# ============================================\nfrom playwright.async_api import async_playwright\nfrom bs4 import BeautifulSoup\n\nasync def scrape_js_quotes():\n    async with async_playwright() as p:\n        # Launch headless browser\n        browser = await p.chromium.launch(headless=True)\n        page = await browser.new_page()\n        \n        # Navigate to the page\n        await page.goto('https://quotes.toscrape.com/js/')\n        \n        # Wait for quotes to load (JavaScript needs time to execute)\n        await page.wait_for_selector('.quote')\n        \n        # Get the rendered HTML\n        html = await page.content()\n        \n        # Close browser\n        await browser.close()\n        \n        return html\n\n# Run the async function in JupyterLab/Notebook\nhtml = await scrape_js_quotes()\n\n# Now parse with BeautifulSoup\nsoup = BeautifulSoup(html, 'html.parser')\nquotes = soup.find_all('div', {'class': 'quote'})\n\nprint(f'Playwright + BeautifulSoup found: {len(quotes)} quotes\\n')\n\n# Display first 3 quotes\nfor quote in quotes[:3]:\n    text = quote.find('span', {'class': 'text'}).text\n    author = quote.find('small', {'class': 'author'}).text\n    print(f'Quote: {text}')\n    print(f'Author: {author}')\n    print('---')\n\n# ============================================\n# SYNC API (for .py scripts) - Uncomment to use\n# ============================================\n# from playwright.sync_api import sync_playwright\n# from bs4 import BeautifulSoup\n#\n# with sync_playwright() as p:\n#     browser = p.chromium.launch(headless=True)\n#     page = browser.new_page()\n#     page.goto('https://quotes.toscrape.com/js/')\n#     page.wait_for_selector('.quote')\n#     html = page.content()\n#     browser.close()\n#\n# soup = BeautifulSoup(html, 'html.parser')\n# quotes = soup.find_all('div', {'class': 'quote'})\n# print(f'Found: {len(quotes)} quotes')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interacting with Pages\n",
    "\n",
    "Playwright can click buttons, fill forms, scroll, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Example: Navigate through multiple pages\n# ============================================\n# ASYNC API (for JupyterLab/Notebook)\n# ============================================\nfrom playwright.async_api import async_playwright\nfrom bs4 import BeautifulSoup\nimport asyncio\n\nasync def scrape_multiple_pages():\n    all_quotes = []\n    \n    async with async_playwright() as p:\n        browser = await p.chromium.launch(headless=True)\n        page = await browser.new_page()\n        \n        await page.goto('https://quotes.toscrape.com/js/')\n        await page.wait_for_selector('.quote')\n        \n        # Scrape multiple pages\n        for page_num in range(1, 4):  # 3 pages\n            print(f'Scraping page {page_num}...')\n            \n            # Get current page content\n            html = await page.content()\n            soup = BeautifulSoup(html, 'html.parser')\n            \n            quotes = soup.find_all('div', {'class': 'quote'})\n            for quote in quotes:\n                all_quotes.append({\n                    'text': quote.find('span', {'class': 'text'}).text,\n                    'author': quote.find('small', {'class': 'author'}).text\n                })\n            \n            # Try to click 'Next' button\n            next_button = await page.query_selector('li.next a')\n            if next_button:\n                await next_button.click()\n                await page.wait_for_selector('.quote')\n                await asyncio.sleep(1)  # Be respectful\n            else:\n                break\n        \n        await browser.close()\n    \n    return all_quotes\n\n# Run the async function in JupyterLab/Notebook\nall_quotes = await scrape_multiple_pages()\nprint(f'\\nTotal quotes collected: {len(all_quotes)}')\n\n# ============================================\n# SYNC API (for .py scripts) - Uncomment to use\n# ============================================\n# from playwright.sync_api import sync_playwright\n# import time\n#\n# all_quotes = []\n#\n# with sync_playwright() as p:\n#     browser = p.chromium.launch(headless=True)\n#     page = browser.new_page()\n#     page.goto('https://quotes.toscrape.com/js/')\n#     page.wait_for_selector('.quote')\n#     \n#     for page_num in range(1, 4):\n#         print(f'Scraping page {page_num}...')\n#         html = page.content()\n#         soup = BeautifulSoup(html, 'html.parser')\n#         \n#         quotes = soup.find_all('div', {'class': 'quote'})\n#         for quote in quotes:\n#             all_quotes.append({\n#                 'text': quote.find('span', {'class': 'text'}).text,\n#                 'author': quote.find('small', {'class': 'author'}).text\n#             })\n#         \n#         next_button = page.query_selector('li.next a')\n#         if next_button:\n#             next_button.click()\n#             page.wait_for_selector('.quote')\n#             time.sleep(1)\n#         else:\n#             break\n#     \n#     browser.close()\n#\n# print(f'\\nTotal quotes collected: {len(all_quotes)}')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrolling for Infinite-Load Pages\n",
    "\n",
    "Some pages load more content as you scroll (\"infinite scroll\"). Here's how to handle that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Example of scrolling (using quotes.toscrape.com/scroll as demo)\n# ============================================\n# ASYNC API (for JupyterLab/Notebook)\n# ============================================\nfrom playwright.async_api import async_playwright\nfrom bs4 import BeautifulSoup\nimport asyncio\n\nasync def scrape_with_scroll():\n    async with async_playwright() as p:\n        browser = await p.chromium.launch(headless=True)\n        page = await browser.new_page()\n        \n        await page.goto('https://quotes.toscrape.com/scroll')\n        await page.wait_for_selector('.quote')\n        \n        # Scroll down multiple times to load more content\n        for i in range(3):\n            # Scroll to bottom\n            await page.evaluate('window.scrollTo(0, document.body.scrollHeight)')\n            await asyncio.sleep(2)  # Wait for content to load\n            print(f'Scrolled {i+1} times')\n        \n        # Get all loaded content\n        html = await page.content()\n        await browser.close()\n        \n        return html\n\n# Run the async function in JupyterLab/Notebook\nhtml = await scrape_with_scroll()\n\nsoup = BeautifulSoup(html, 'html.parser')\nquotes = soup.find_all('div', {'class': 'quote'})\nprint(f'\\nTotal quotes after scrolling: {len(quotes)}')\n\n# ============================================\n# SYNC API (for .py scripts) - Uncomment to use\n# ============================================\n# from playwright.sync_api import sync_playwright\n# import time\n#\n# with sync_playwright() as p:\n#     browser = p.chromium.launch(headless=True)\n#     page = browser.new_page()\n#     \n#     page.goto('https://quotes.toscrape.com/scroll')\n#     page.wait_for_selector('.quote')\n#     \n#     for i in range(3):\n#         page.evaluate('window.scrollTo(0, document.body.scrollHeight)')\n#         time.sleep(2)\n#         print(f'Scrolled {i+1} times')\n#     \n#     html = page.content()\n#     soup = BeautifulSoup(html, 'html.parser')\n#     quotes = soup.find_all('div', {'class': 'quote'})\n#     \n#     print(f'\\nTotal quotes after scrolling: {len(quotes)}')\n#     \n#     browser.close()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try It Yourself!\n",
    "\n",
    "Use Playwright to:\n",
    "1. Visit a JavaScript-rendered page of your choice\n",
    "2. Wait for specific content to load\n",
    "3. Extract and display the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 5: APIs - The Preferred Approach\n",
    "\n",
    "## Why APIs Are Better Than Scraping\n",
    "\n",
    "| Aspect | API | Web Scraping |\n",
    "|--------|-----|-------------|\n",
    "| **Data Format** | Structured (JSON/XML) | Unstructured HTML |\n",
    "| **Reliability** | Stable endpoints | Pages can change anytime |\n",
    "| **Legality** | Clear terms of service | Often gray area |\n",
    "| **Rate Limiting** | Documented limits | Risk of being blocked |\n",
    "| **Data Quality** | Clean, complete | May need extensive cleaning |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REST API Basics\n",
    "\n",
    "**REST APIs** (Representational State Transfer) are the most common type:\n",
    "\n",
    "- Use HTTP methods: GET (read), POST (create), PUT (update), DELETE\n",
    "- Return data in JSON format\n",
    "- Have documented endpoints (URLs)\n",
    "- May require authentication (API keys)\n",
    "\n",
    "### Making API Requests with `requests`\n",
    "\n",
    "```python\n",
    "import requests\n",
    "\n",
    "# Simple GET request\n",
    "response = requests.get('https://api.example.com/data')\n",
    "data = response.json()  # Parse JSON response\n",
    "\n",
    "# GET with parameters\n",
    "params = {'city': 'Edinburgh', 'units': 'metric'}\n",
    "response = requests.get('https://api.example.com/weather', params=params)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Open-Meteo Weather API (No API Key Required)\n",
    "\n",
    "Open-Meteo provides free weather data without requiring registration or API keys (endpoint) - perfect for learning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Edinburgh coordinates\n",
    "latitude = 55.95\n",
    "longitude = -3.19\n",
    "\n",
    "# Build the API URL\n",
    "url = \"https://api.open-meteo.com/v1/forecast\"\n",
    "params = {\n",
    "    \"latitude\": latitude,\n",
    "    \"longitude\": longitude,\n",
    "    \"current_weather\": True,\n",
    "    \"hourly\": \"temperature_2m,precipitation\",\n",
    "    \"timezone\": \"Europe/London\"\n",
    "}\n",
    "\n",
    "# Make the request\n",
    "response = requests.get(url, params=params)\n",
    "\n",
    "# Check if successful\n",
    "print(f'Status Code: {response.status_code}')\n",
    "\n",
    "# Parse JSON response\n",
    "data = response.json()\n",
    "print(f'\\nResponse keys: {data.keys()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract current weather\n",
    "current = data['current_weather']\n",
    "\n",
    "print('Current Weather in Edinburgh:')\n",
    "print(f\"  Temperature: {current['temperature']}Â°C\")\n",
    "print(f\"  Wind Speed: {current['windspeed']} km/h\")\n",
    "print(f\"  Time: {current['time']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get hourly forecast\n",
    "import pandas as pd\n",
    "\n",
    "hourly_df = pd.DataFrame({\n",
    "    'time': data['hourly']['time'],\n",
    "    'temperature': data['hourly']['temperature_2m'],\n",
    "    'precipitation': data['hourly']['precipitation']\n",
    "})\n",
    "\n",
    "# Show first 24 hours\n",
    "hourly_df.head(24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch Weather for Multiple Cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scottish cities coordinates\n",
    "cities = {\n",
    "    'Edinburgh': (55.95, -3.19),\n",
    "    'Glasgow': (55.86, -4.25),\n",
    "    'Aberdeen': (57.15, -2.11),\n",
    "    'Dundee': (56.46, -2.97)\n",
    "}\n",
    "\n",
    "weather_data = []\n",
    "\n",
    "for city, (lat, lon) in cities.items():\n",
    "    params = {\n",
    "        \"latitude\": lat,\n",
    "        \"longitude\": lon,\n",
    "        \"current_weather\": True\n",
    "    }\n",
    "    \n",
    "    response = requests.get(url, params=params)\n",
    "    data = response.json()\n",
    "    \n",
    "    weather_data.append({\n",
    "        'city': city,\n",
    "        'temperature': data['current_weather']['temperature'],\n",
    "        'windspeed': data['current_weather']['windspeed']\n",
    "    })\n",
    "\n",
    "weather_df = pd.DataFrame(weather_data)\n",
    "weather_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: JSONPlaceholder (Free Test API)\n",
    "\n",
    "JSONPlaceholder is a free fake API for testing and prototyping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get fake posts\n",
    "response = requests.get('https://jsonplaceholder.typicode.com/posts')\n",
    "posts = response.json()\n",
    "\n",
    "print(f'Number of posts: {len(posts)}')\n",
    "print(f'\\nFirst post:')\n",
    "print(f\"  Title: {posts[0]['title']}\")\n",
    "print(f\"  Body: {posts[0]['body'][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get users\n",
    "response = requests.get('https://jsonplaceholder.typicode.com/users')\n",
    "users = response.json()\n",
    "\n",
    "users_df = pd.DataFrame(users)[['id', 'name', 'email', 'company']]\n",
    "users_df['company'] = users_df['company'].apply(lambda x: x['name'])\n",
    "users_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Google Maps Places API (Advanced - Optional)\n",
    "\n",
    "**Note**: This requires an API key from Google Cloud Platform. The free tier allows limited requests.\n",
    "\n",
    "### Getting an API Key\n",
    "\n",
    "1. Go to [Google Cloud Console](https://console.cloud.google.com/)\n",
    "2. Create a new project\n",
    "3. Enable \"Places API\"\n",
    "4. Create credentials > API Key\n",
    "5. Restrict your key to only Places API\n",
    "\n",
    "**Important**: Google's free tier includes 5 reviews per place, and has monthly quota limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example code for Google Maps API (requires API key)\n",
    "# Uncomment and add your API key to use\n",
    "\n",
    "'''\n",
    "import requests\n",
    "\n",
    "api_key = 'YOUR_API_KEY_HERE'\n",
    "place_id = 'ChIJ98CZIJrHh0gRWApM5esemkY'  # Edinburgh Castle\n",
    "\n",
    "url = f'https://maps.googleapis.com/maps/api/place/details/json'\n",
    "params = {\n",
    "    'place_id': place_id,\n",
    "    'fields': 'name,rating,reviews',\n",
    "    'key': api_key\n",
    "}\n",
    "\n",
    "response = requests.get(url, params=params)\n",
    "data = response.json()\n",
    "\n",
    "if data['status'] == 'OK':\n",
    "    result = data['result']\n",
    "    print(f\"Place: {result['name']}\")\n",
    "    print(f\"Rating: {result['rating']}\")\n",
    "    print(f\"\\nReviews (max 5):\")\n",
    "    for review in result.get('reviews', []):\n",
    "        print(f\"  - {review['author_name']}: {review['rating']}/5\")\n",
    "        print(f\"    {review['text'][:100]}...\")\n",
    "else:\n",
    "    print(f\"Error: {data['status']}\")\n",
    "'''\n",
    "\n",
    "print('Google Maps API example - requires API key')\n",
    "print('See comments above for implementation details')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the googlemaps Library\n",
    "\n",
    "For easier Google Maps API access, you can use the official Python library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install googlemaps library (uncomment to install)\n",
    "# !pip install googlemaps\n",
    "\n",
    "'''\n",
    "import googlemaps\n",
    "\n",
    "gmaps = googlemaps.Client(key='YOUR_API_KEY_HERE')\n",
    "\n",
    "# Search for a place\n",
    "result = gmaps.places('Edinburgh Castle')\n",
    "place_id = result['results'][0]['place_id']\n",
    "\n",
    "# Get place details including reviews\n",
    "place = gmaps.place(place_id)\n",
    "reviews = place['result'].get('reviews', [])\n",
    "\n",
    "print(f\"Found {len(reviews)} reviews (API limit: 5)\")\n",
    "'''\n",
    "\n",
    "print('Google Maps library example - requires API key')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Summary and Best Practices\n",
    "\n",
    "## Decision Checklist\n",
    "\n",
    "1. **Always check for an API first** - it's the cleanest solution\n",
    "2. **For static HTML** - use `requests` + `BeautifulSoup`\n",
    "3. **For JavaScript-rendered content** - use `Playwright`\n",
    "\n",
    "## Best Practices\n",
    "\n",
    "### Legal & Ethical\n",
    "- Check `robots.txt` (e.g., `https://example.com/robots.txt`)\n",
    "- Read the website's Terms of Service\n",
    "- Don't scrape personal data without consent\n",
    "\n",
    "### Technical\n",
    "- Add delays between requests (`time.sleep(1)`)\n",
    "- Handle errors gracefully (`try/except`)\n",
    "- Use appropriate User-Agent headers\n",
    "- Cache results to avoid repeated requests\n",
    "\n",
    "### Code Quality\n",
    "- Store data in structured formats (CSV, JSON)\n",
    "- Document your scraping logic\n",
    "- Test with small samples first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Good scraping practices template\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "def scrape_with_best_practices(url):\n",
    "    \"\"\"Example of responsible web scraping.\"\"\"\n",
    "    \n",
    "    # Use a descriptive User-Agent\n",
    "    headers = {\n",
    "        'User-Agent': 'Educational Web Scraper (University Project)'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Make request\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()  # Raise error for bad status codes\n",
    "        \n",
    "        # Parse content\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Be respectful - wait before next request\n",
    "        time.sleep(1)\n",
    "        \n",
    "        return soup\n",
    "        \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f'Error fetching {url}: {e}')\n",
    "        return None\n",
    "\n",
    "# Test the function\n",
    "soup = scrape_with_best_practices('https://quotes.toscrape.com/')\n",
    "if soup:\n",
    "    title = soup.find('title').text\n",
    "    print(f'Successfully scraped: {title}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## What's Next?\n",
    "\n",
    "- Complete the exercises in `Week1-Exercise.ipynb`\n",
    "- Try the assessment preparation challenge\n",
    "- (Optional) Explore Selenium in `Week1-optional-selenium.ipynb`\n",
    "\n",
    "---\n",
    "\n",
    "*End of Week 1 Notes*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}