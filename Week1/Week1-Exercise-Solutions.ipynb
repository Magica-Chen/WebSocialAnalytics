{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1: Exercise Solutions\n",
    "\n",
    "**Web and Social Network Analytics**\n",
    "\n",
    "---\n",
    "\n",
    "This notebook contains complete solutions for all exercises. Try to solve them yourself first before looking at these solutions!\n",
    "\n",
    "**Disclaimer**: This educational content is provided for instructional purposes only. Always respect website terms of service and legal requirements when scraping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Web scraping\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# Data handling\n",
    "import pandas as pd\n",
    "\n",
    "print('Libraries imported successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Exercise 1 Solution: BeautifulSoup Basics\n",
    "\n",
    "**Task**: Extract the SCQF Level from a DRPS course page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution for Exercise 1\n",
    "\n",
    "# Step 1: Fetch the page\n",
    "url = 'http://www.drps.ed.ac.uk/24-25/dpt/cxcmse11427.htm'\n",
    "html = urlopen(url)\n",
    "\n",
    "# Step 2: Parse with BeautifulSoup\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# Step 3: Find the table containing course information\n",
    "table = soup.find('table', {'class': 'sitstablegrid'})\n",
    "\n",
    "# Step 4: Search for the cell containing 'SCQF Level'\n",
    "if table:\n",
    "    for cell in table.find_all('td'):\n",
    "        if 'SCQF Level' in cell.text:\n",
    "            print('Found:', cell.text.strip())\n",
    "            break\n",
    "else:\n",
    "    print('Table not found')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative Solution: More Specific Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: Find both SCQF Level and Credits\n",
    "\n",
    "url = 'http://www.drps.ed.ac.uk/24-25/dpt/cxcmse11427.htm'\n",
    "html = urlopen(url)\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# Get course name\n",
    "course_name = soup.find('h1')\n",
    "if course_name:\n",
    "    print(f'Course: {course_name.text.strip()}')\n",
    "\n",
    "# Find all cells and extract specific info\n",
    "table = soup.find('table', {'class': 'sitstablegrid'})\n",
    "if table:\n",
    "    for cell in table.find_all('td'):\n",
    "        text = cell.text.strip()\n",
    "        if 'SCQF Level' in text:\n",
    "            print(f'SCQF Level: {text}')\n",
    "        elif 'SCQF Credits' in text:\n",
    "            print(f'Credits: {text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Mistakes to Avoid\n",
    "\n",
    "1. **Not checking if element exists**: Always check `if table:` before using `.find_all()`\n",
    "2. **Forgetting to strip whitespace**: Use `.strip()` to clean text\n",
    "3. **Using wrong class name**: Inspect the page carefully to get exact class names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Exercise 2 Solution: Multi-Item Scraping\n",
    "\n",
    "**Task**: Scrape quotes from multiple pages of quotes.toscrape.com."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution for Exercise 2\n",
    "\n",
    "all_quotes = []\n",
    "\n",
    "# Loop through pages 1 to 3\n",
    "for page_num in range(1, 4):\n",
    "    url = f'https://quotes.toscrape.com/page/{page_num}/'\n",
    "    print(f'Scraping page {page_num}...')\n",
    "    \n",
    "    # Fetch and parse the page\n",
    "    html = urlopen(url)\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    # Find all quote containers\n",
    "    quotes = soup.find_all('div', {'class': 'quote'})\n",
    "    \n",
    "    # Extract data from each quote\n",
    "    for quote in quotes:\n",
    "        # Get the quote text\n",
    "        text = quote.find('span', {'class': 'text'}).text\n",
    "        \n",
    "        # Get the author\n",
    "        author = quote.find('small', {'class': 'author'}).text\n",
    "        \n",
    "        # Get all tags as a list\n",
    "        tags = [tag.text for tag in quote.find_all('a', {'class': 'tag'})]\n",
    "        \n",
    "        # Append to our list\n",
    "        all_quotes.append({\n",
    "            'text': text,\n",
    "            'author': author,\n",
    "            'tags': tags\n",
    "        })\n",
    "    \n",
    "    print(f'  Found {len(quotes)} quotes')\n",
    "    \n",
    "    # Be respectful - wait between requests\n",
    "    time.sleep(1)\n",
    "\n",
    "print(f'\\nTotal quotes collected: {len(all_quotes)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and display DataFrame\n",
    "df = pd.DataFrame(all_quotes)\n",
    "print(f'DataFrame shape: {df.shape}')\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bonus: Analyze the data\n",
    "print('\\nQuotes by author:')\n",
    "print(df['author'].value_counts().head())\n",
    "\n",
    "# Flatten tags and count\n",
    "all_tags = [tag for tags_list in df['tags'] for tag in tags_list]\n",
    "print('\\nMost common tags:')\n",
    "print(pd.Series(all_tags).value_counts().head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative: Using requests instead of urlopen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative solution using requests library\n",
    "\n",
    "import requests\n",
    "\n",
    "all_quotes_v2 = []\n",
    "\n",
    "for page_num in range(1, 4):\n",
    "    url = f'https://quotes.toscrape.com/page/{page_num}/'\n",
    "    \n",
    "    # Use requests.get() instead of urlopen()\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check if request was successful\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        for quote in soup.find_all('div', {'class': 'quote'}):\n",
    "            all_quotes_v2.append({\n",
    "                'text': quote.find('span', {'class': 'text'}).text,\n",
    "                'author': quote.find('small', {'class': 'author'}).text,\n",
    "                'tags': [t.text for t in quote.find_all('a', {'class': 'tag'})]\n",
    "            })\n",
    "    \n",
    "    time.sleep(1)\n",
    "\n",
    "print(f'Collected {len(all_quotes_v2)} quotes using requests')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Exercise 3 Solution: Dynamic Content with Playwright\n",
    "\n",
    "**Task**: Scrape the JavaScript-rendered quotes page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, demonstrate that BeautifulSoup alone doesn't work\n",
    "url = 'https://quotes.toscrape.com/js/'\n",
    "html = urlopen(url)\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "quotes_bs = soup.find_all('div', {'class': 'quote'})\n",
    "print(f'BeautifulSoup alone finds: {len(quotes_bs)} quotes')\n",
    "print('(Expected: 0, because quotes are loaded by JavaScript)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Solution using Playwright\n# ============================================\n# ASYNC API (for JupyterLab/Notebook)\n# ============================================\nfrom playwright.async_api import async_playwright\nfrom bs4 import BeautifulSoup\nimport pandas as pd\n\nurl = 'https://quotes.toscrape.com/js/'\n\nasync def scrape_js_quotes():\n    async with async_playwright() as p:\n        # Launch browser in headless mode\n        browser = await p.chromium.launch(headless=True)\n        page = await browser.new_page()\n        \n        # Navigate to the page\n        print('Loading page...')\n        await page.goto(url)\n        \n        # Wait for quotes to be loaded by JavaScript\n        await page.wait_for_selector('.quote')\n        print('Quotes loaded!')\n        \n        # Get the rendered HTML\n        html = await page.content()\n        \n        # Close browser\n        await browser.close()\n        \n        return html\n\n# Run the async function in JupyterLab/Notebook\nhtml = await scrape_js_quotes()\n\n# Now parse with BeautifulSoup\nsoup = BeautifulSoup(html, 'html.parser')\nquotes = soup.find_all('div', {'class': 'quote'})\n\nprint(f'\\nPlaywright + BeautifulSoup finds: {len(quotes)} quotes')\n\n# Extract data\njs_quotes = []\nfor quote in quotes:\n    js_quotes.append({\n        'text': quote.find('span', {'class': 'text'}).text,\n        'author': quote.find('small', {'class': 'author'}).text\n    })\n\n# Create DataFrame\njs_df = pd.DataFrame(js_quotes)\njs_df\n\n# ============================================\n# SYNC API (for .py scripts) - Uncomment to use\n# ============================================\n# from playwright.sync_api import sync_playwright\n#\n# with sync_playwright() as p:\n#     browser = p.chromium.launch(headless=True)\n#     page = browser.new_page()\n#     page.goto(url)\n#     page.wait_for_selector('.quote')\n#     html = page.content()\n#     browser.close()\n#\n# soup = BeautifulSoup(html, 'html.parser')\n# quotes = soup.find_all('div', {'class': 'quote'})\n# print(f'Found: {len(quotes)} quotes')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative: Scrape Multiple Pages with Playwright"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Advanced: Scrape multiple pages using Playwright\n# ============================================\n# ASYNC API (for JupyterLab/Notebook)\n# ============================================\nfrom playwright.async_api import async_playwright\nfrom bs4 import BeautifulSoup\nimport asyncio\n\nasync def scrape_multiple_js_pages():\n    all_js_quotes = []\n    \n    async with async_playwright() as p:\n        browser = await p.chromium.launch(headless=True)\n        page = await browser.new_page()\n        \n        # Start at first page\n        await page.goto('https://quotes.toscrape.com/js/')\n        await page.wait_for_selector('.quote')\n        \n        for page_num in range(1, 4):  # 3 pages\n            print(f'Scraping page {page_num}...')\n            \n            # Get current page content\n            html = await page.content()\n            soup = BeautifulSoup(html, 'html.parser')\n            \n            quotes = soup.find_all('div', {'class': 'quote'})\n            for quote in quotes:\n                all_js_quotes.append({\n                    'text': quote.find('span', {'class': 'text'}).text,\n                    'author': quote.find('small', {'class': 'author'}).text\n                })\n            \n            print(f'  Found {len(quotes)} quotes')\n            \n            # Try to click Next button\n            next_btn = await page.query_selector('li.next a')\n            if next_btn:\n                await next_btn.click()\n                await page.wait_for_selector('.quote')\n                await asyncio.sleep(1)\n            else:\n                print('No more pages')\n                break\n        \n        await browser.close()\n    \n    return all_js_quotes\n\n# Run the async function in JupyterLab/Notebook\nall_js_quotes = await scrape_multiple_js_pages()\nprint(f'\\nTotal quotes from JS pages: {len(all_js_quotes)}')\n\n# ============================================\n# SYNC API (for .py scripts) - Uncomment to use\n# ============================================\n# from playwright.sync_api import sync_playwright\n# import time\n#\n# all_js_quotes = []\n#\n# with sync_playwright() as p:\n#     browser = p.chromium.launch(headless=True)\n#     page = browser.new_page()\n#     page.goto('https://quotes.toscrape.com/js/')\n#     page.wait_for_selector('.quote')\n#     \n#     for page_num in range(1, 4):\n#         print(f'Scraping page {page_num}...')\n#         html = page.content()\n#         soup = BeautifulSoup(html, 'html.parser')\n#         \n#         quotes = soup.find_all('div', {'class': 'quote'})\n#         for quote in quotes:\n#             all_js_quotes.append({\n#                 'text': quote.find('span', {'class': 'text'}).text,\n#                 'author': quote.find('small', {'class': 'author'}).text\n#             })\n#         \n#         next_btn = page.query_selector('li.next a')\n#         if next_btn:\n#             next_btn.click()\n#             page.wait_for_selector('.quote')\n#             time.sleep(1)\n#         else:\n#             break\n#     \n#     browser.close()\n#\n# print(f'Total quotes: {len(all_js_quotes)}')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Exercise 4A Solution: Weather API\n",
    "\n",
    "**Task**: Fetch weather data for Scottish cities using Open-Meteo API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution for Exercise 4A\n",
    "\n",
    "import requests\n",
    "\n",
    "# Define cities and their coordinates\n",
    "cities = {\n",
    "    'Edinburgh': (55.95, -3.19),\n",
    "    'Glasgow': (55.86, -4.25),\n",
    "    'Aberdeen': (57.15, -2.11)\n",
    "}\n",
    "\n",
    "# API endpoint\n",
    "api_url = 'https://api.open-meteo.com/v1/forecast'\n",
    "\n",
    "weather_data = []\n",
    "\n",
    "for city, (lat, lon) in cities.items():\n",
    "    print(f'Fetching weather for {city}...')\n",
    "    \n",
    "    # Build request parameters\n",
    "    params = {\n",
    "        'latitude': lat,\n",
    "        'longitude': lon,\n",
    "        'current_weather': True\n",
    "    }\n",
    "    \n",
    "    # Make API request\n",
    "    response = requests.get(api_url, params=params)\n",
    "    \n",
    "    # Check if successful\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        current = data['current_weather']\n",
    "        \n",
    "        weather_data.append({\n",
    "            'city': city,\n",
    "            'temperature': current['temperature'],\n",
    "            'windspeed': current['windspeed'],\n",
    "            'time': current['time']\n",
    "        })\n",
    "    else:\n",
    "        print(f'  Error: {response.status_code}')\n",
    "\n",
    "# Create DataFrame\n",
    "weather_df = pd.DataFrame(weather_data)\n",
    "print('\\nWeather Data:')\n",
    "weather_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bonus: Add more weather details\n",
    "\n",
    "detailed_weather = []\n",
    "\n",
    "for city, (lat, lon) in cities.items():\n",
    "    params = {\n",
    "        'latitude': lat,\n",
    "        'longitude': lon,\n",
    "        'current_weather': True,\n",
    "        'hourly': 'temperature_2m,precipitation_probability',\n",
    "        'timezone': 'Europe/London'\n",
    "    }\n",
    "    \n",
    "    response = requests.get(api_url, params=params)\n",
    "    data = response.json()\n",
    "    \n",
    "    # Get current weather\n",
    "    current = data['current_weather']\n",
    "    \n",
    "    # Get next hour's precipitation probability\n",
    "    hourly = data.get('hourly', {})\n",
    "    precip_prob = hourly.get('precipitation_probability', [0])[0]\n",
    "    \n",
    "    detailed_weather.append({\n",
    "        'city': city,\n",
    "        'temperature_c': current['temperature'],\n",
    "        'windspeed_kmh': current['windspeed'],\n",
    "        'precipitation_probability_%': precip_prob\n",
    "    })\n",
    "\n",
    "detailed_df = pd.DataFrame(detailed_weather)\n",
    "print('Detailed Weather:')\n",
    "detailed_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Exercise 4B Solution: Google Maps API (Optional)\n",
    "\n",
    "**Task**: Fetch place details using Google Maps API.\n",
    "\n",
    "**Note**: This requires a valid API key from Google Cloud Platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution for Exercise 4B (requires API key)\n",
    "\n",
    "'''\n",
    "# Uncomment and add your API key to use this code\n",
    "\n",
    "import requests\n",
    "\n",
    "api_key = 'YOUR_API_KEY_HERE'  # Replace with your key\n",
    "\n",
    "# Edinburgh Castle Place ID\n",
    "place_id = 'ChIJ98CZIJrHh0gRWApM5esemkY'\n",
    "\n",
    "# API endpoint\n",
    "url = 'https://maps.googleapis.com/maps/api/place/details/json'\n",
    "\n",
    "# Parameters\n",
    "params = {\n",
    "    'place_id': place_id,\n",
    "    'fields': 'name,rating,user_ratings_total,reviews',\n",
    "    'key': api_key\n",
    "}\n",
    "\n",
    "# Make request\n",
    "response = requests.get(url, params=params)\n",
    "data = response.json()\n",
    "\n",
    "if data['status'] == 'OK':\n",
    "    result = data['result']\n",
    "    \n",
    "    print(f\"Place: {result['name']}\")\n",
    "    print(f\"Rating: {result.get('rating', 'N/A')}\")\n",
    "    print(f\"Total Reviews: {result.get('user_ratings_total', 'N/A')}\")\n",
    "    \n",
    "    print(\"\\nFirst 3 Reviews:\")\n",
    "    reviews = result.get('reviews', [])\n",
    "    for i, review in enumerate(reviews[:3]):\n",
    "        print(f\"\\n{i+1}. {review['author_name']} - {review['rating']}/5 stars\")\n",
    "        print(f\"   {review['text'][:150]}...\")\n",
    "else:\n",
    "    print(f\"Error: {data['status']}\")\n",
    "'''\n",
    "\n",
    "print('Google Maps API solution (requires API key)')\n",
    "print('Uncomment the code above and add your API key to run')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the googlemaps Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative solution using googlemaps library\n",
    "\n",
    "'''\n",
    "# First install: pip install googlemaps\n",
    "import googlemaps\n",
    "\n",
    "api_key = 'YOUR_API_KEY_HERE'\n",
    "gmaps = googlemaps.Client(key=api_key)\n",
    "\n",
    "# Search for Edinburgh Castle\n",
    "places_result = gmaps.places('Edinburgh Castle')\n",
    "\n",
    "if places_result['results']:\n",
    "    place_id = places_result['results'][0]['place_id']\n",
    "    \n",
    "    # Get detailed info\n",
    "    place_details = gmaps.place(place_id)\n",
    "    result = place_details['result']\n",
    "    \n",
    "    print(f\"Name: {result['name']}\")\n",
    "    print(f\"Rating: {result.get('rating', 'N/A')}\")\n",
    "    \n",
    "    reviews = result.get('reviews', [])\n",
    "    print(f\"\\nNumber of reviews returned: {len(reviews)} (API limit: 5)\")\n",
    "    \n",
    "    # Create DataFrame of reviews\n",
    "    if reviews:\n",
    "        reviews_df = pd.DataFrame([{\n",
    "            'author': r['author_name'],\n",
    "            'rating': r['rating'],\n",
    "            'text': r['text'][:100] + '...',\n",
    "            'time': r['relative_time_description']\n",
    "        } for r in reviews])\n",
    "        print(reviews_df)\n",
    "'''\n",
    "\n",
    "print('googlemaps library solution (requires API key)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Bonus Challenge Solution\n",
    "\n",
    "**Task**: Create a function that combines API and web scraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bonus Solution: Combining techniques\n",
    "\n",
    "def get_city_info(city_name, latitude, longitude):\n",
    "    \"\"\"\n",
    "    Get comprehensive information about a city.\n",
    "    \n",
    "    Combines:\n",
    "    - Open-Meteo API for weather\n",
    "    - Web scraping for Wikipedia summary\n",
    "    \n",
    "    Args:\n",
    "        city_name: Name of the city\n",
    "        latitude: City latitude\n",
    "        longitude: City longitude\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with city information\n",
    "    \"\"\"\n",
    "    info = {'city': city_name}\n",
    "    \n",
    "    # 1. Get weather from API\n",
    "    try:\n",
    "        weather_url = 'https://api.open-meteo.com/v1/forecast'\n",
    "        weather_params = {\n",
    "            'latitude': latitude,\n",
    "            'longitude': longitude,\n",
    "            'current_weather': True\n",
    "        }\n",
    "        weather_response = requests.get(weather_url, params=weather_params)\n",
    "        weather_data = weather_response.json()\n",
    "        \n",
    "        current = weather_data['current_weather']\n",
    "        info['temperature_c'] = current['temperature']\n",
    "        info['windspeed_kmh'] = current['windspeed']\n",
    "        info['weather_time'] = current['time']\n",
    "    except Exception as e:\n",
    "        info['weather_error'] = str(e)\n",
    "    \n",
    "    # 2. Get Wikipedia summary (scraping)\n",
    "    try:\n",
    "        wiki_url = f'https://en.wikipedia.org/wiki/{city_name}'\n",
    "        wiki_response = requests.get(wiki_url)\n",
    "        wiki_soup = BeautifulSoup(wiki_response.text, 'html.parser')\n",
    "        \n",
    "        # Get first paragraph\n",
    "        paragraphs = wiki_soup.find_all('p')\n",
    "        for p in paragraphs:\n",
    "            text = p.text.strip()\n",
    "            if len(text) > 100:  # Skip short paragraphs\n",
    "                info['wikipedia_summary'] = text[:300] + '...'\n",
    "                break\n",
    "    except Exception as e:\n",
    "        info['wikipedia_error'] = str(e)\n",
    "    \n",
    "    return info\n",
    "\n",
    "# Test the function\n",
    "edinburgh_info = get_city_info('Edinburgh', 55.95, -3.19)\n",
    "\n",
    "print('Edinburgh City Information:')\n",
    "print('=' * 50)\n",
    "for key, value in edinburgh_info.items():\n",
    "    if key == 'wikipedia_summary':\n",
    "        print(f'\\n{key}:')\n",
    "        print(f'  {value}')\n",
    "    else:\n",
    "        print(f'{key}: {value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply to multiple cities\n",
    "cities = {\n",
    "    'Edinburgh': (55.95, -3.19),\n",
    "    'Glasgow': (55.86, -4.25),\n",
    "    'Aberdeen': (57.15, -2.11)\n",
    "}\n",
    "\n",
    "all_city_info = []\n",
    "for city, (lat, lon) in cities.items():\n",
    "    print(f'Processing {city}...')\n",
    "    info = get_city_info(city, lat, lon)\n",
    "    all_city_info.append(info)\n",
    "    time.sleep(1)  # Be respectful\n",
    "\n",
    "# Create summary DataFrame (weather only for cleaner display)\n",
    "weather_summary = pd.DataFrame([{\n",
    "    'city': info['city'],\n",
    "    'temperature_c': info.get('temperature_c', 'N/A'),\n",
    "    'windspeed_kmh': info.get('windspeed_kmh', 'N/A')\n",
    "} for info in all_city_info])\n",
    "\n",
    "print('\\nWeather Summary:')\n",
    "weather_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n# Summary of Key Patterns\n\n## BeautifulSoup Pattern\n```python\nfrom urllib.request import urlopen\nfrom bs4 import BeautifulSoup\n\nhtml = urlopen(url)\nsoup = BeautifulSoup(html, 'html.parser')\nelements = soup.find_all('tag', {'class': 'classname'})\n```\n\n## Playwright Pattern\n\n### For JupyterLab/Notebook (Async API)\n```python\nfrom playwright.async_api import async_playwright\n\nasync def scrape_page():\n    async with async_playwright() as p:\n        browser = await p.chromium.launch(headless=True)\n        page = await browser.new_page()\n        await page.goto(url)\n        await page.wait_for_selector('.selector')\n        html = await page.content()\n        await browser.close()\n        return html\n\nhtml = await scrape_page()\n```\n\n### For .py Scripts (Sync API)\n```python\nfrom playwright.sync_api import sync_playwright\n\nwith sync_playwright() as p:\n    browser = p.chromium.launch(headless=True)\n    page = browser.new_page()\n    page.goto(url)\n    page.wait_for_selector('.selector')\n    html = page.content()\n    browser.close()\n```\n\n## API Pattern\n```python\nimport requests\n\nresponse = requests.get(url, params=params)\ndata = response.json()\n```\n\n---\n\n*End of Exercise Solutions*"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}