{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4: Exercise Solutions - Unsupervised Learning Techniques\n",
    "\n",
    "**Web and Social Network Analytics**\n",
    "\n",
    "---\n",
    "\n",
    "This notebook contains complete solutions for all exercises. **Try to solve them yourself first!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "\n",
    "# Sentiment Analysis\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print('All libraries imported successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 1 Solution: Sentiment Analysis with VADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Define the reviews\n",
    "reviews = [\n",
    "    \"This product is absolutely AMAZING! Best purchase ever!!!\",\n",
    "    \"Meh, it's okay. Nothing special.\",\n",
    "    \"Terrible quality. Completely disappointed :(\",\n",
    "    \"Pretty good value for the price, would recommend.\",\n",
    "    \"DO NOT BUY! Worst experience of my life!!!\"\n",
    "]\n",
    "\n",
    "# Step 2: Initialize VADER analyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Step 3: Define classification function\n",
    "def classify_sentiment(compound):\n",
    "    \"\"\"Classify sentiment based on compound score.\n",
    "    \n",
    "    VADER recommends these thresholds:\n",
    "    - Positive: compound > 0.05\n",
    "    - Negative: compound < -0.05\n",
    "    - Neutral: -0.05 <= compound <= 0.05\n",
    "    \"\"\"\n",
    "    if compound > 0.05:\n",
    "        return \"Positive\"\n",
    "    elif compound < -0.05:\n",
    "        return \"Negative\"\n",
    "    else:\n",
    "        return \"Neutral\"\n",
    "\n",
    "# Step 4: Analyze each review\n",
    "print(\"Sentiment Analysis Results\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'Review':<45} {'Compound':>10} {'Class':>12}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "pos_count, neu_count, neg_count = 0, 0, 0\n",
    "\n",
    "for review in reviews:\n",
    "    # Get VADER scores\n",
    "    scores = analyzer.polarity_scores(review)\n",
    "    compound = scores['compound']\n",
    "    \n",
    "    # Classify\n",
    "    classification = classify_sentiment(compound)\n",
    "    \n",
    "    # Count\n",
    "    if classification == \"Positive\":\n",
    "        pos_count += 1\n",
    "    elif classification == \"Negative\":\n",
    "        neg_count += 1\n",
    "    else:\n",
    "        neu_count += 1\n",
    "    \n",
    "    # Truncate long reviews for display\n",
    "    display = review[:42] + \"...\" if len(review) > 45 else review\n",
    "    print(f\"{display:<45} {compound:>10.3f} {classification:>12}\")\n",
    "\n",
    "# Step 5: Print summary\n",
    "print(\"-\" * 70)\n",
    "print(f\"\\nSummary: {pos_count} Positive, {neu_count} Neutral, {neg_count} Negative\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Insights**:\n",
    "- VADER correctly identifies strong positive (\"AMAZING\") and negative (\"Terrible\", \"Worst\") reviews\n",
    "- Capitalization and punctuation (!!!) amplify sentiment scores\n",
    "- Emoticons like `:(`  are recognized as negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 2 Solution: Jaccard Similarity Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Define user purchases\n",
    "users = {\n",
    "    'Alice': {'iPhone', 'AirPods', 'MacBook', 'iPad'},\n",
    "    'Bob': {'iPhone', 'AirPods', 'Galaxy Watch'},\n",
    "    'Carol': {'MacBook', 'iPad', 'iMac'},\n",
    "    'Dave': {'iPhone', 'AirPods', 'MacBook', 'iPad', 'iMac'}\n",
    "}\n",
    "\n",
    "# Step 2: Implement Jaccard similarity function\n",
    "def jaccard_similarity(set1, set2):\n",
    "    \"\"\"Calculate Jaccard similarity between two sets.\n",
    "    \n",
    "    Formula: J(A,B) = |A ∩ B| / |A ∪ B|\n",
    "    \n",
    "    Returns a value between 0 (no overlap) and 1 (identical sets).\n",
    "    \"\"\"\n",
    "    intersection = len(set1 & set2)  # & is set intersection\n",
    "    union = len(set1 | set2)         # | is set union\n",
    "    return intersection / union if union > 0 else 0\n",
    "\n",
    "# Step 3: Calculate similarity for all pairs\n",
    "print(\"Jaccard Similarities:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'Pair':<20} {'Intersection':<15} {'Union':<10} {'Jaccard':>8}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "similarities = {}\n",
    "user_names = list(users.keys())\n",
    "\n",
    "for user1, user2 in combinations(user_names, 2):\n",
    "    set1, set2 = users[user1], users[user2]\n",
    "    intersection = set1 & set2\n",
    "    union = set1 | set2\n",
    "    sim = jaccard_similarity(set1, set2)\n",
    "    similarities[(user1, user2)] = sim\n",
    "    \n",
    "    print(f\"{user1}-{user2:<14} {str(intersection):<15} {len(union):<10} {sim:>8.3f}\")\n",
    "\n",
    "# Step 4: Find most and least similar pairs\n",
    "most_similar = max(similarities, key=similarities.get)\n",
    "least_similar = min(similarities, key=similarities.get)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"Most similar pair: {most_similar[0]} & {most_similar[1]}\")\n",
    "print(f\"  Jaccard similarity: {similarities[most_similar]:.3f}\")\n",
    "print(f\"  Common products: {users[most_similar[0]] & users[most_similar[1]]}\")\n",
    "\n",
    "print(f\"\\nLeast similar pair: {least_similar[0]} & {least_similar[1]}\")\n",
    "print(f\"  Jaccard similarity: {similarities[least_similar]:.3f}\")\n",
    "print(f\"  Common products: {users[least_similar[0]] & users[least_similar[1]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Insights**:\n",
    "- Alice and Dave are most similar (4 products in common out of 5 total = 0.8)\n",
    "- Bob and Carol are least similar (only 0 products in common = 0.0)\n",
    "- Jaccard considers both what users bought AND what they didn't buy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 3 Solution: Support, Confidence, and Lift Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Define transactions\n",
    "transactions = [\n",
    "    ['bread', 'milk', 'eggs'],\n",
    "    ['bread', 'butter'],\n",
    "    ['milk', 'eggs', 'butter'],\n",
    "    ['bread', 'milk', 'eggs', 'butter'],\n",
    "    ['bread', 'milk'],\n",
    "    ['eggs', 'butter'],\n",
    "    ['bread', 'milk', 'butter'],\n",
    "    ['bread', 'eggs']\n",
    "]\n",
    "\n",
    "print(f\"Total transactions: {len(transactions)}\")\n",
    "for i, t in enumerate(transactions, 1):\n",
    "    print(f\"  T{i}: {t}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Implement support function\n",
    "def support(itemset, transactions):\n",
    "    \"\"\"Calculate the support of an itemset.\n",
    "    \n",
    "    Support = (# transactions containing itemset) / (total # transactions)\n",
    "    \"\"\"\n",
    "    # Handle single item as string\n",
    "    if isinstance(itemset, str):\n",
    "        itemset = [itemset]\n",
    "    \n",
    "    count = 0\n",
    "    for trans in transactions:\n",
    "        # Check if all items in itemset are in the transaction\n",
    "        if set(itemset).issubset(set(trans)):\n",
    "            count += 1\n",
    "    return count / len(transactions)\n",
    "\n",
    "# Step 3: Calculate individual item supports\n",
    "print(\"Individual Item Support:\")\n",
    "print(\"-\" * 40)\n",
    "items = ['bread', 'milk', 'eggs', 'butter']\n",
    "for item in items:\n",
    "    sup = support(item, transactions)\n",
    "    count = int(sup * len(transactions))\n",
    "    print(f\"  support({item}) = {count}/{len(transactions)} = {sup:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Calculate support for {bread, milk}\n",
    "print(\"\\nPair Support:\")\n",
    "print(\"-\" * 40)\n",
    "sup_bread_milk = support(['bread', 'milk'], transactions)\n",
    "print(f\"  support({{bread, milk}}) = {sup_bread_milk:.3f}\")\n",
    "print(f\"  Appears in {int(sup_bread_milk * len(transactions))} out of {len(transactions)} transactions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Calculate confidence values\n",
    "print(\"\\nConfidence:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Confidence(bread -> milk) = support({bread, milk}) / support({bread})\n",
    "conf_bread_milk = support(['bread', 'milk'], transactions) / support('bread', transactions)\n",
    "print(f\"  confidence(bread -> milk) = {sup_bread_milk:.3f} / {support('bread', transactions):.3f} = {conf_bread_milk:.3f}\")\n",
    "print(f\"  Interpretation: {conf_bread_milk*100:.0f}% of customers who buy bread also buy milk\")\n",
    "\n",
    "# Confidence(milk -> bread) = support({bread, milk}) / support({milk})\n",
    "conf_milk_bread = support(['bread', 'milk'], transactions) / support('milk', transactions)\n",
    "print(f\"\\n  confidence(milk -> bread) = {sup_bread_milk:.3f} / {support('milk', transactions):.3f} = {conf_milk_bread:.3f}\")\n",
    "print(f\"  Interpretation: {conf_milk_bread*100:.0f}% of customers who buy milk also buy bread\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Calculate lift\n",
    "print(\"\\nLift:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Lift(bread -> milk) = support({bread, milk}) / (support({bread}) * support({milk}))\n",
    "sup_bread = support('bread', transactions)\n",
    "sup_milk = support('milk', transactions)\n",
    "lift_bread_milk = sup_bread_milk / (sup_bread * sup_milk)\n",
    "\n",
    "print(f\"  lift(bread -> milk) = {sup_bread_milk:.3f} / ({sup_bread:.3f} * {sup_milk:.3f})\")\n",
    "print(f\"                      = {sup_bread_milk:.3f} / {sup_bread * sup_milk:.3f}\")\n",
    "print(f\"                      = {lift_bread_milk:.3f}\")\n",
    "\n",
    "# Step 7: Interpret results\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"Interpretation:\")\n",
    "print(\"=\"*40)\n",
    "if lift_bread_milk > 1:\n",
    "    print(f\"  Lift = {lift_bread_milk:.3f} > 1\")\n",
    "    print(f\"  Bread and milk are POSITIVELY ASSOCIATED\")\n",
    "    print(f\"  Customers who buy bread are {lift_bread_milk:.1f}x more likely to buy milk\")\n",
    "    print(f\"  than expected if purchases were independent.\")\n",
    "elif lift_bread_milk < 1:\n",
    "    print(f\"  Lift = {lift_bread_milk:.3f} < 1\")\n",
    "    print(f\"  Bread and milk are SUBSTITUTES\")\n",
    "else:\n",
    "    print(f\"  Lift = {lift_bread_milk:.3f} = 1\")\n",
    "    print(f\"  Bread and milk are INDEPENDENT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 4 Solution: A-Priori Algorithm Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Implement mingle function\n",
    "def mingle(items, level):\n",
    "    \"\"\"Generate candidate itemsets of size 'level' from items.\n",
    "    \n",
    "    For level 2: combines single items into pairs\n",
    "    For level 3+: combines itemsets into larger sets\n",
    "    \n",
    "    Returns frozensets to allow adding to a set.\n",
    "    \"\"\"\n",
    "    outcome = set()\n",
    "    \n",
    "    for item in items:\n",
    "        for item2 in items:\n",
    "            if item != item2:\n",
    "                new_combination = set()\n",
    "                \n",
    "                if level > 2:\n",
    "                    # Combine existing itemsets (which are iterable)\n",
    "                    for i in item:\n",
    "                        new_combination.add(i)\n",
    "                    for i in item2:\n",
    "                        new_combination.add(i)\n",
    "                else:\n",
    "                    # Combine single items\n",
    "                    new_combination.add(item)\n",
    "                    new_combination.add(item2)\n",
    "                \n",
    "                # Only keep if it's the right size\n",
    "                if len(new_combination) == level:\n",
    "                    outcome.add(frozenset(new_combination))\n",
    "    \n",
    "    return outcome\n",
    "\n",
    "# Test mingle\n",
    "assert mingle([\"a\",\"b\",\"c\"], 2) == {frozenset({'a', 'c'}), \n",
    "                                     frozenset({'b', 'c'}), \n",
    "                                     frozenset({'a', 'b'})}\n",
    "print(\"mingle() test passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Implement support function for levels\n",
    "def support_level(itemset, transactions, level):\n",
    "    \"\"\"Calculate support of an itemset at a given level.\n",
    "    \n",
    "    Level 1: itemset is a single item (string)\n",
    "    Level 2+: itemset is a collection of items\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    \n",
    "    for trans in transactions:\n",
    "        contain = True\n",
    "        \n",
    "        if level > 1:\n",
    "            # Check each item in the itemset\n",
    "            for item in itemset:\n",
    "                if item not in trans:\n",
    "                    contain = False\n",
    "                    break\n",
    "        else:\n",
    "            # Single item check\n",
    "            if itemset not in trans:\n",
    "                contain = False\n",
    "        \n",
    "        if contain:\n",
    "            count += 1\n",
    "    \n",
    "    return count / len(transactions)\n",
    "\n",
    "# Test support\n",
    "test_trans = [[\"a\",\"b\",\"c\"], [\"a\",\"b\",\"d\"], [\"b\",\"c\"], [\"a\",\"c\"]]\n",
    "assert support_level(\"a\", test_trans, 1) == 0.75\n",
    "assert support_level([\"a\",\"b\"], test_trans, 2) == 0.5\n",
    "print(\"support() tests passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Implement apriori function\n",
    "def apriori(level, transactions, items, minsup):\n",
    "    \"\"\"A-Priori algorithm implementation.\n",
    "    \n",
    "    1. Calculate support for all items at current level\n",
    "    2. Keep only items meeting minimum support\n",
    "    3. Generate candidates for next level\n",
    "    4. Recurse until no more candidates\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Level {level}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    retain = set()\n",
    "    \n",
    "    # Find items meeting minimum support\n",
    "    for item in items:\n",
    "        sup = support_level(item, transactions, level)\n",
    "        status = \"KEEP\" if sup >= minsup else \"DROP\"\n",
    "        print(f\"  {str(item):25} support: {sup:.2f}  [{status}]\")\n",
    "        \n",
    "        if sup >= minsup:\n",
    "            retain.add(item)\n",
    "    \n",
    "    print(f\"\\nRetained: {retain}\")\n",
    "    \n",
    "    # Move to next level\n",
    "    level += 1\n",
    "    newsets = mingle(retain, level)\n",
    "    print(f\"New candidates for level {level}: {newsets}\")\n",
    "    \n",
    "    # Recurse if there are new candidates\n",
    "    if len(newsets) != 0 and level < len(items) + 1:\n",
    "        apriori(level, transactions, newsets, minsup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Load baskets data and run A-Priori\n",
    "file = open('data/baskets.csv', 'r')\n",
    "\n",
    "transactions = []\n",
    "items = set()\n",
    "\n",
    "for line in file:\n",
    "    line = line.replace('\\n', '')\n",
    "    litems = line.split(',')\n",
    "    transactions.append(litems)\n",
    "    for item in litems:\n",
    "        items.add(item)\n",
    "\n",
    "file.close()\n",
    "\n",
    "print(f\"Loaded {len(transactions)} transactions\")\n",
    "print(f\"Unique items: {items}\")\n",
    "print(\"\\nTransactions:\")\n",
    "for i, t in enumerate(transactions, 1):\n",
    "    print(f\"  T{i}: {t}\")\n",
    "\n",
    "# Run A-Priori with minSup = 60%\n",
    "print(\"\\n\" + \"#\"*50)\n",
    "print(\"A-PRIORI ALGORITHM (minSup = 60%)\")\n",
    "print(\"#\"*50)\n",
    "apriori(1, transactions, items, 0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Insights**:\n",
    "- The A-Priori algorithm uses the \"anti-monotone\" property: if an itemset doesn't meet minSup, none of its supersets will\n",
    "- This allows efficient pruning of the search space\n",
    "- The algorithm terminates when no new candidates can be generated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 5 Solution: Collaborative Filtering Recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load ratings data\n",
    "ratings = pd.read_csv('data/ratings.csv')\n",
    "ratings = ratings[:5000]  # Sample for speed\n",
    "\n",
    "noMovies = len(ratings['movieId'].unique())\n",
    "noUsers = len(ratings['userId'].unique())\n",
    "\n",
    "print(f\"Dataset: {noMovies} movies rated by {noUsers} users\")\n",
    "print(f\"Total ratings: {len(ratings)}\")\n",
    "print(ratings.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Create utility matrix\n",
    "utility = np.zeros(shape=(noUsers, noMovies))\n",
    "\n",
    "# Map movie IDs to sequential indices\n",
    "# (Movie IDs are not sequential, so we need a mapping)\n",
    "movieIds = {}\n",
    "for i, mid in enumerate(ratings['movieId'].unique()):\n",
    "    movieIds[mid] = i\n",
    "\n",
    "# Reverse mapping for later use\n",
    "movieIds_reverse = {v: k for k, v in movieIds.items()}\n",
    "\n",
    "# Populate the matrix\n",
    "for _, row in ratings.iterrows():\n",
    "    uid = int(row['userId']) - 1  # User IDs are 1-indexed\n",
    "    mid = movieIds[row['movieId']]\n",
    "    utility[uid, mid] = row['rating']\n",
    "\n",
    "print(f\"Utility matrix shape: {utility.shape}\")\n",
    "print(f\"Non-zero entries: {(utility != 0).sum()}\")\n",
    "print(f\"Sparsity: {(utility == 0).sum() / utility.size * 100:.1f}% empty\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Implement findSimilarUsers\n",
    "def findSimilarUsers(person_number, utility_matrix, minCos=0.5):\n",
    "    \"\"\"Find users similar to the given user using cosine similarity.\n",
    "    \n",
    "    Args:\n",
    "        person_number: Index of the target user\n",
    "        utility_matrix: User-item rating matrix\n",
    "        minCos: Minimum similarity threshold\n",
    "    \n",
    "    Returns:\n",
    "        List of (user_id, similarity) tuples, sorted by similarity\n",
    "    \"\"\"\n",
    "    similar_users = []\n",
    "    target_ratings = utility_matrix[person_number]\n",
    "    \n",
    "    for other in range(len(utility_matrix)):\n",
    "        if person_number != other:\n",
    "            other_ratings = utility_matrix[other]\n",
    "            \n",
    "            # Skip if either user has no ratings (avoid division by zero)\n",
    "            if np.any(target_ratings) and np.any(other_ratings):\n",
    "                # scipy.cosine returns DISTANCE, so similarity = 1 - distance\n",
    "                cos_sim = 1 - cosine(target_ratings, other_ratings)\n",
    "                \n",
    "                if cos_sim > minCos:\n",
    "                    similar_users.append((other, cos_sim))\n",
    "    \n",
    "    # Sort by similarity (highest first)\n",
    "    return sorted(similar_users, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Test with user 0\n",
    "similar = findSimilarUsers(0, utility, minCos=0.3)\n",
    "print(f\"Found {len(similar)} similar users for User 0\")\n",
    "print(\"\\nTop 5 similar users:\")\n",
    "for uid, sim in similar[:5]:\n",
    "    print(f\"  User {uid}: similarity = {sim:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Implement findNewProducts\n",
    "def findNewProducts(similar_users, person_number, utility_matrix, minScore=2.0):\n",
    "    \"\"\"Recommend movies based on similar users' ratings.\n",
    "    \n",
    "    For each movie the target user hasn't rated:\n",
    "    - Calculate average rating from similar users who rated it\n",
    "    - Recommend if average exceeds minScore threshold\n",
    "    \n",
    "    Returns:\n",
    "        List of (movie_index, predicted_score) tuples, sorted by score\n",
    "    \"\"\"\n",
    "    recommendations = []\n",
    "    \n",
    "    for movie in range(utility_matrix.shape[1]):\n",
    "        # Only consider movies the user hasn't rated\n",
    "        if utility_matrix[person_number, movie] == 0:\n",
    "            scores = []\n",
    "            \n",
    "            # Collect ratings from similar users\n",
    "            for user_id, sim in similar_users:\n",
    "                rating = utility_matrix[user_id, movie]\n",
    "                if rating > 0:  # Similar user has rated this movie\n",
    "                    scores.append(rating)\n",
    "            \n",
    "            # Calculate average if we have ratings\n",
    "            if scores:\n",
    "                avg_score = sum(scores) / len(scores)\n",
    "                if avg_score > minScore:\n",
    "                    recommendations.append((movie, avg_score))\n",
    "    \n",
    "    # Sort by predicted score (highest first)\n",
    "    return sorted(recommendations, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Generate recommendations for user 0\n",
    "recs = findNewProducts(similar, 0, utility, minScore=3.5)\n",
    "\n",
    "print(f\"\\nTop 10 recommendations for User 0 (minScore=3.5):\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'Movie Index':<15} {'Predicted Score':>15}\")\n",
    "print(\"-\" * 50)\n",
    "for movie_idx, score in recs[:10]:\n",
    "    print(f\"{movie_idx:<15} {score:>15.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bonus: Run for multiple users\n",
    "print(\"\\nRecommendation Summary:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for user_id in range(min(5, noUsers)):\n",
    "    similar = findSimilarUsers(user_id, utility, minCos=0.3)\n",
    "    recs = findNewProducts(similar, user_id, utility, minScore=3.5)\n",
    "    \n",
    "    print(f\"User {user_id}: {len(similar)} similar users, {len(recs)} recommendations\")\n",
    "    if recs:\n",
    "        top_movie, top_score = recs[0]\n",
    "        print(f\"  Top recommendation: Movie {top_movie} (score: {top_score:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Insights**:\n",
    "- User-based collaborative filtering finds users with similar taste\n",
    "- The utility matrix is very sparse (most users haven't rated most movies)\n",
    "- Cosine similarity measures angle between rating vectors, ignoring magnitude\n",
    "- Higher similarity thresholds give fewer but more reliable recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Bonus Solution: K-Means Clustering on Starbucks Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load and filter Starbucks data\n",
    "data = pd.read_csv(\"data/starbucks_locations.csv\", index_col=0)\n",
    "data = data.dropna()\n",
    "\n",
    "# Filter to Middle East region for faster processing\n",
    "filtered = data[(data[\"Latitude\"].between(24, 27)) & \n",
    "                (data[\"Longitude\"].between(49, 56))]\n",
    "\n",
    "print(f\"Total locations: {len(data)}\")\n",
    "print(f\"Filtered locations: {len(filtered)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Try different values of K\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "k_values = [3, 5, 10]\n",
    "inertias = []\n",
    "\n",
    "for i, k in enumerate(k_values):\n",
    "    kmeans = KMeans(n_clusters=k, max_iter=500, random_state=42)\n",
    "    kmeans.fit(filtered)\n",
    "    inertias.append(kmeans.inertia_)\n",
    "    \n",
    "    ax = axes[i]\n",
    "    ax.scatter(filtered['Longitude'], filtered['Latitude'], \n",
    "               c=kmeans.labels_, cmap='tab10', s=50, alpha=0.7)\n",
    "    ax.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], \n",
    "               c='red', marker='X', s=200, label='Centroids')\n",
    "    ax.set_xlabel('Longitude')\n",
    "    ax.set_ylabel('Latitude')\n",
    "    ax.set_title(f'K-Means (K={k})')\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Elbow method to find optimal K\n",
    "k_range = range(2, 15)\n",
    "inertias = []\n",
    "\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, max_iter=500, random_state=42)\n",
    "    kmeans.fit(filtered)\n",
    "    inertias.append(kmeans.inertia_)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(k_range, inertias, 'bo-')\n",
    "plt.xlabel('Number of Clusters (K)')\n",
    "plt.ylabel('Inertia (Within-cluster sum of squares)')\n",
    "plt.title('Elbow Method for Optimal K')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nDiscussion: How to choose optimal K?\")\n",
    "print(\"=\"*50)\n",
    "print(\"1. Elbow Method: Look for the 'elbow' where adding more\")\n",
    "print(\"   clusters doesn't significantly reduce inertia.\")\n",
    "print(\"2. Silhouette Score: Measures how similar points are to\")\n",
    "print(\"   their own cluster vs other clusters.\")\n",
    "print(\"3. Domain Knowledge: How many meaningful groups exist?\")\n",
    "print(\"4. Business Requirements: What's practical to manage?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### Exercise 1: Sentiment Analysis\n",
    "- VADER is effective for social media text\n",
    "- Compound score ranges from -1 (negative) to +1 (positive)\n",
    "- Standard thresholds: > 0.05 positive, < -0.05 negative\n",
    "\n",
    "### Exercise 2: Jaccard Similarity\n",
    "- Measures overlap between sets\n",
    "- J(A,B) = |A intersection B| / |A union B|\n",
    "- Range: 0 (no overlap) to 1 (identical)\n",
    "\n",
    "### Exercise 3: Association Rules\n",
    "- Support: How often does the itemset appear?\n",
    "- Confidence: How often is the rule correct?\n",
    "- Lift > 1 means positive association\n",
    "\n",
    "### Exercise 4: A-Priori Algorithm\n",
    "- Efficiently finds frequent itemsets\n",
    "- Uses anti-monotone property for pruning\n",
    "- minSup controls the threshold\n",
    "\n",
    "### Exercise 5: Collaborative Filtering\n",
    "- Finds similar users based on rating patterns\n",
    "- Recommends items that similar users liked\n",
    "- Cosine similarity is common for rating vectors"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
