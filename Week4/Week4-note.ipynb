{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4: Unsupervised Learning Techniques\n",
    "\n",
    "**Web and Social Network Analytics**\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. **Explain** the difference between supervised and unsupervised learning\n",
    "2. **Apply** sentiment analysis to classify customer reviews\n",
    "3. **Calculate** support, confidence, and lift for association rules\n",
    "4. **Implement** the A-Priori algorithm to find frequent itemsets\n",
    "5. **Design** a collaborative filtering recommendation system\n",
    "\n",
    "---\n",
    "\n",
    "**ShopSocial Context**: How can we understand customer opinions, find product bundles, and make personalized recommendations?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Run this cell first to import all required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# Sentiment Analysis\n",
    "try:\n",
    "    from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "    print(\"VADER imported successfully!\")\n",
    "except ImportError:\n",
    "    print(\"VADER not installed. Run: pip install vaderSentiment\")\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "print('All libraries imported successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: Clustering Review\n",
    "\n",
    "Clustering groups similar items together **without predefined labels**. This is the essence of unsupervised learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Distance Metrics\n",
    "\n",
    "Clustering algorithms group data based on distance. Common metrics include:\n",
    "\n",
    "- **Euclidean Distance**: $d = \\sqrt{\\sum_{i}(a_i - b_i)^2}$\n",
    "- **Manhattan Distance**: $d = \\sum_{i}|a_i - b_i|$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 K-Means Clustering\n",
    "\n",
    "K-Means partitions data into K clusters by:\n",
    "1. Randomly initialize K centroids\n",
    "2. Assign each point to the nearest centroid\n",
    "3. Update centroids to the mean of assigned points\n",
    "4. Repeat until convergence\n",
    "\n",
    "**Key Parameter**: You must specify K (number of clusters) in advance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Starbucks location data\n",
    "data = pd.read_csv(\"data/starbucks_locations.csv\", index_col=0)\n",
    "data = data.dropna()\n",
    "print(f\"Total locations: {len(data)}\")\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to a specific region for faster processing\n",
    "# Middle East region (UAE, Saudi Arabia, etc.)\n",
    "region_data = data[(data[\"Longitude\"].between(49, 56)) & (data[\"Latitude\"].between(24, 27))]\n",
    "print(f\"Filtered locations: {len(region_data)}\")\n",
    "region_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply K-Means with K=5\n",
    "kmeans = KMeans(n_clusters=5, max_iter=500, random_state=42)\n",
    "kmeans.fit(region_data)\n",
    "\n",
    "print(f\"Cluster centroids:\")\n",
    "for i, center in enumerate(kmeans.cluster_centers_):\n",
    "    print(f\"  Cluster {i}: Longitude={center[0]:.2f}, Latitude={center[1]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize K-Means clusters\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(region_data['Longitude'], region_data['Latitude'], \n",
    "            c=kmeans.labels_, cmap='tab10', s=50, alpha=0.7)\n",
    "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], \n",
    "            c='red', marker='X', s=200, label='Centroids')\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.title('K-Means Clustering of Starbucks Locations (K=5)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 DBSCAN Clustering\n",
    "\n",
    "DBSCAN (Density-Based Spatial Clustering of Applications with Noise) finds clusters of arbitrary shape and identifies **outliers**.\n",
    "\n",
    "**Key Parameters**:\n",
    "- `eps`: Maximum distance between points in a cluster\n",
    "- `min_samples`: Minimum points to form a dense region\n",
    "\n",
    "**Advantage**: Does not require specifying K in advance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply DBSCAN\n",
    "dbscan = DBSCAN(eps=0.1, min_samples=3)\n",
    "dbscan.fit(region_data)\n",
    "\n",
    "labels = set(dbscan.labels_)\n",
    "n_clusters = len(labels) - (1 if -1 in labels else 0)\n",
    "n_outliers = list(dbscan.labels_).count(-1)\n",
    "\n",
    "print(f\"Number of clusters: {n_clusters}\")\n",
    "print(f\"Number of outliers: {n_outliers}\")\n",
    "print(f\"Cluster labels: {labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize DBSCAN clusters (outliers shown as black X)\n",
    "plt.figure(figsize=(10, 8))\n",
    "colors = dbscan.labels_\n",
    "\n",
    "# Plot clusters\n",
    "for i, (lon, lat) in enumerate(zip(region_data['Longitude'], region_data['Latitude'])):\n",
    "    if colors[i] == -1:\n",
    "        plt.plot(lon, lat, 'kx', markersize=8)  # Outliers in black\n",
    "    else:\n",
    "        plt.scatter(lon, lat, c=[colors[i]], cmap='tab10', s=50, vmin=0, vmax=max(colors))\n",
    "\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.title('DBSCAN Clustering (Black X = Outliers)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 RFM Customer Segmentation\n",
    "\n",
    "**RFM Analysis** segments customers using three metrics:\n",
    "- **R**ecency: How recently did the customer purchase?\n",
    "- **F**requency: How often do they purchase?\n",
    "- **M**onetary: How much do they spend?\n",
    "\n",
    "| Segment | Recency | Frequency | Monetary | Strategy |\n",
    "|---------|---------|-----------|----------|----------|\n",
    "| **Champions** | Recent | Often | High | Reward loyalty |\n",
    "| **At Risk** | Long ago | Often | High | Win back campaigns |\n",
    "| **New Customers** | Recent | Low | Low | Onboarding emails |\n",
    "| **Hibernating** | Long ago | Low | Low | Re-engagement offers |\n",
    "\n",
    "**Key Insight**: K-Means on RFM features helps ShopSocial personalize marketing without manual labeling!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: Sentiment Analysis\n",
    "\n",
    "**Sentiment Analysis** automatically determines the emotional tone of text (positive, negative, or neutral)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Why Sentiment Analysis Matters\n",
    "\n",
    "**ShopSocial Challenge**: With 500,000 product reviews, a human reading 1 review per minute would take **347 days** (24/7) to read them all!\n",
    "\n",
    "Automated sentiment analysis can:\n",
    "- Identify unhappy customers for follow-up\n",
    "- Track product quality over time\n",
    "- Compare sentiment across product categories\n",
    "- Detect emerging issues before they escalate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Sentiment Analysis Approaches\n",
    "\n",
    "| Approach | How It Works | Pros | Cons |\n",
    "|----------|--------------|------|------|\n",
    "| **Lexicon-Based** | Count positive/negative words using a dictionary | Simple, interpretable | Misses context, sarcasm |\n",
    "| **VADER** | Social media optimized lexicon | Handles emoji, slang, caps | Still rule-based |\n",
    "| **Machine Learning** | Train classifier on labeled examples | More accurate | Needs training data |\n",
    "| **LLM-Based** | Prompt GPT/Claude directly | Flexible, contextual | API costs, latency |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Lexicon-Based Sentiment Analysis\n",
    "\n",
    "The simplest approach: build a dictionary of words with sentiment scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple sentiment lexicon\n",
    "sentiment_lexicon = {\n",
    "    'excellent': 3, 'amazing': 3, 'love': 2, 'great': 2,\n",
    "    'good': 1, 'nice': 1, 'okay': 0, 'fine': 0,\n",
    "    'bad': -1, 'poor': -1, 'disappointing': -2,\n",
    "    'terrible': -3, 'awful': -3, 'hate': -2\n",
    "}\n",
    "\n",
    "def lexicon_sentiment(text, lexicon):\n",
    "    \"\"\"Calculate sentiment score using a lexicon.\"\"\"\n",
    "    words = text.lower().split()\n",
    "    score = 0\n",
    "    matched_words = []\n",
    "    \n",
    "    for word in words:\n",
    "        # Remove punctuation\n",
    "        clean_word = ''.join(c for c in word if c.isalpha())\n",
    "        if clean_word in lexicon:\n",
    "            score += lexicon[clean_word]\n",
    "            matched_words.append(f\"{clean_word}({lexicon[clean_word]:+d})\")\n",
    "    \n",
    "    return score, matched_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Worked example from lecture\n",
    "review = \"This product is excellent! The quality is good but shipping was bad.\"\n",
    "\n",
    "score, matched = lexicon_sentiment(review, sentiment_lexicon)\n",
    "print(f\"Review: {review}\")\n",
    "print(f\"Matched words: {matched}\")\n",
    "print(f\"Total score: {score}\")\n",
    "print(f\"Sentiment: {'Positive' if score > 0 else 'Negative' if score < 0 else 'Neutral'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 VADER Sentiment Analysis\n",
    "\n",
    "**VADER** (Valence Aware Dictionary for Sentiment Reasoning) is specifically designed for social media text. It handles:\n",
    "- Capitalization (\"AMAZING\" vs \"amazing\")\n",
    "- Punctuation (\"good!!!\" vs \"good\")\n",
    "- Emoji and emoticons\n",
    "- Slang and abbreviations\n",
    "\n",
    "Returns a **compound score** from -1 (most negative) to +1 (most positive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize VADER\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Test reviews\n",
    "reviews = [\n",
    "    \"This product is AMAZING!!!\",\n",
    "    \"Meh, it's okay I guess...\",\n",
    "    \"Worst purchase ever :(\",\n",
    "    \"Pretty good value for the price\",\n",
    "    \"DO NOT BUY! Terrible quality!!!\"\n",
    "]\n",
    "\n",
    "print(\"VADER Sentiment Analysis\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Review':<40} {'Compound':>10} {'Sentiment':>10}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for review in reviews:\n",
    "    scores = analyzer.polarity_scores(review)\n",
    "    compound = scores['compound']\n",
    "    \n",
    "    # Classify based on compound score\n",
    "    if compound > 0.05:\n",
    "        sentiment = \"Positive\"\n",
    "    elif compound < -0.05:\n",
    "        sentiment = \"Negative\"\n",
    "    else:\n",
    "        sentiment = \"Neutral\"\n",
    "    \n",
    "    # Truncate long reviews for display\n",
    "    display = review[:37] + \"...\" if len(review) > 40 else review\n",
    "    print(f\"{display:<40} {compound:>10.3f} {sentiment:>10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Challenges in Sentiment Analysis\n",
    "\n",
    "| Challenge | Example | Problem |\n",
    "|-----------|---------|--------|\n",
    "| **Negation** | \"This is **not** good\" | Simple lexicon says positive! |\n",
    "| **Sarcasm** | \"Oh **great**, another delayed delivery\" | Actually negative |\n",
    "| **Context** | \"This phone battery **dies** quickly\" | \"dies\" not about death |\n",
    "| **Domain-specific** | \"This lens is **sharp**\" | Positive for cameras! |\n",
    "| **Comparative** | \"Better than X but worse than Y\" | Mixed sentiment |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VADER handles some challenges well\n",
    "challenging_reviews = [\n",
    "    \"This is NOT good\",           # Negation\n",
    "    \"The product is good!!!\",     # Emphasis\n",
    "    \"The product is GOOD\",        # Capitalization\n",
    "]\n",
    "\n",
    "print(\"VADER handling challenges:\")\n",
    "for review in challenging_reviews:\n",
    "    scores = analyzer.polarity_scores(review)\n",
    "    print(f\"  '{review}' -> compound: {scores['compound']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 3: Frequent Itemset Analysis\n",
    "\n",
    "**Market Basket Analysis** finds items that are frequently purchased together.\n",
    "\n",
    "**Example**: \"Customers who bought iPhone also bought AirPods\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Association Rules\n",
    "\n",
    "An **association rule** has the form: **Antecedent -> Consequent**\n",
    "\n",
    "Example: `{Beer, Pizza} -> {Diapers}`\n",
    "\n",
    "We measure the strength of rules using three metrics:\n",
    "1. **Support**: How often does the itemset appear?\n",
    "2. **Confidence**: How often is the rule correct?\n",
    "3. **Lift**: Is there a real association or just coincidence?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Support\n",
    "\n",
    "**Support** measures how frequently an itemset appears in transactions:\n",
    "\n",
    "$$sup(A) = \\frac{|\\{A \\subseteq t | t \\in T\\}|}{|T|}$$\n",
    "\n",
    "Where:\n",
    "- $A$ is an itemset (e.g., {iPhone, AirPods})\n",
    "- $T$ is the set of all transactions\n",
    "- $t$ is a single transaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example transactions (from lecture)\n",
    "transactions = [\n",
    "    ['iphoneX', 'S8', 'LG55', 'S9'],      # Basket 1\n",
    "    ['iphoneX', 'S8', 'S9', 'LG55'],      # Basket 2  \n",
    "    ['LG55', 'S2', 'iphoneX'],            # Basket 3\n",
    "    ['iphoneX', 'S8', 'LG55']             # Basket 4\n",
    "]\n",
    "\n",
    "def support(itemset, transactions):\n",
    "    \"\"\"Calculate the support of an itemset.\"\"\"\n",
    "    if isinstance(itemset, str):\n",
    "        itemset = [itemset]\n",
    "    \n",
    "    count = 0\n",
    "    for trans in transactions:\n",
    "        if set(itemset).issubset(set(trans)):\n",
    "            count += 1\n",
    "    return count / len(transactions)\n",
    "\n",
    "# Calculate support for individual items\n",
    "items = ['iphoneX', 'S8', 'LG55', 'S9', 'S2']\n",
    "print(\"Support for individual items:\")\n",
    "for item in items:\n",
    "    sup = support(item, transactions)\n",
    "    print(f\"  support({item}) = {sup:.2f} ({int(sup*4)}/4 transactions)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Confidence\n",
    "\n",
    "**Confidence** measures how often the rule is correct:\n",
    "\n",
    "$$conf(A \\rightarrow B) = \\frac{sup(A \\cap B)}{sup(A)}$$\n",
    "\n",
    "\"Of the transactions containing A, what fraction also contain B?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confidence(A, B, transactions):\n",
    "    \"\"\"Calculate confidence of rule A -> B.\"\"\"\n",
    "    sup_A = support(A, transactions)\n",
    "    sup_AB = support(list(set([A] if isinstance(A, str) else A) | \n",
    "                         set([B] if isinstance(B, str) else B)), transactions)\n",
    "    return sup_AB / sup_A if sup_A > 0 else 0\n",
    "\n",
    "# Example: iphoneX -> S8\n",
    "conf = confidence('iphoneX', 'S8', transactions)\n",
    "print(f\"Confidence(iphoneX -> S8) = {conf:.2f}\")\n",
    "print(f\"  = support({{iphoneX, S8}}) / support({{iphoneX}})\")\n",
    "print(f\"  = {support(['iphoneX', 'S8'], transactions):.2f} / {support('iphoneX', transactions):.2f}\")\n",
    "print(f\"\\nInterpretation: {conf*100:.0f}% of customers who bought iphoneX also bought S8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Lift\n",
    "\n",
    "**Lift** measures whether there's a real association:\n",
    "\n",
    "$$lift(A \\rightarrow B) = \\frac{sup(A \\cap B)}{sup(A) \\times sup(B)}$$\n",
    "\n",
    "**Interpretation**:\n",
    "- **Lift > 1**: Items are **dependent** (buy together more than expected)\n",
    "- **Lift = 1**: Items are **independent** (no association)\n",
    "- **Lift < 1**: Items are **substitutes** (buy one OR the other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lift(A, B, transactions):\n",
    "    \"\"\"Calculate lift of rule A -> B.\"\"\"\n",
    "    sup_A = support(A, transactions)\n",
    "    sup_B = support(B, transactions)\n",
    "    sup_AB = support(list(set([A] if isinstance(A, str) else A) | \n",
    "                         set([B] if isinstance(B, str) else B)), transactions)\n",
    "    return sup_AB / (sup_A * sup_B) if sup_A * sup_B > 0 else 0\n",
    "\n",
    "# Calculate lift for iphoneX -> S8\n",
    "lift_val = lift('iphoneX', 'S8', transactions)\n",
    "print(f\"Lift(iphoneX -> S8) = {lift_val:.2f}\")\n",
    "print(f\"  = support({{iphoneX, S8}}) / (support({{iphoneX}}) * support({{S8}}))\")\n",
    "print(f\"  = {support(['iphoneX', 'S8'], transactions):.2f} / ({support('iphoneX', transactions):.2f} * {support('S8', transactions):.2f})\")\n",
    "\n",
    "if lift_val > 1:\n",
    "    print(f\"\\nInterpretation: Lift > 1, so iphoneX and S8 are DEPENDENT (bought together)\")\n",
    "elif lift_val < 1:\n",
    "    print(f\"\\nInterpretation: Lift < 1, so iphoneX and S8 are SUBSTITUTES\")\n",
    "else:\n",
    "    print(f\"\\nInterpretation: Lift = 1, so iphoneX and S8 are INDEPENDENT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 A-Priori Algorithm\n",
    "\n",
    "The **A-Priori algorithm** efficiently finds all frequent itemsets:\n",
    "\n",
    "1. Find support of all 1-item sets\n",
    "2. Keep only those meeting minimum support (minSup)\n",
    "3. Generate 2-item candidate sets from survivors\n",
    "4. Repeat until no more candidates\n",
    "\n",
    "**Key Insight**: If an itemset doesn't meet minSup, none of its supersets will either!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mingle(items, level):\n",
    "    \"\"\"Generate candidate itemsets of size 'level' from items.\"\"\"\n",
    "    outcome = set()\n",
    "    for item in items:\n",
    "        for item2 in items:\n",
    "            if item != item2:\n",
    "                new_combination = set()\n",
    "                if level > 2:  # Combine existing itemsets\n",
    "                    for i in item:\n",
    "                        new_combination.add(i)\n",
    "                    for i in item2:\n",
    "                        new_combination.add(i)\n",
    "                else:  # Combine single items\n",
    "                    new_combination.add(item)\n",
    "                    new_combination.add(item2)\n",
    "                \n",
    "                if len(new_combination) == level:\n",
    "                    outcome.add(frozenset(new_combination))\n",
    "    return outcome\n",
    "\n",
    "def support_level(itemset, transactions, level):\n",
    "    \"\"\"Calculate support for itemsets at any level.\"\"\"\n",
    "    count = 0\n",
    "    for trans in transactions:\n",
    "        contain = True\n",
    "        if level > 1:\n",
    "            for item in itemset:\n",
    "                if item not in trans:\n",
    "                    contain = False\n",
    "                    break\n",
    "        else:\n",
    "            if itemset not in trans:\n",
    "                contain = False\n",
    "        if contain:\n",
    "            count += 1\n",
    "    return count / len(transactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apriori(level, transactions, items, minsup):\n",
    "    \"\"\"A-Priori algorithm implementation.\"\"\"\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Level {level}:\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    retain = set()\n",
    "    \n",
    "    # Calculate support for each item\n",
    "    for item in items:\n",
    "        sup = support_level(item, transactions, level)\n",
    "        status = \"KEEP\" if sup >= minsup else \"DROP\"\n",
    "        print(f\"  {str(item):30} support: {sup:.2f}  [{status}]\")\n",
    "        if sup >= minsup:\n",
    "            retain.add(item)\n",
    "    \n",
    "    print(f\"\\nRetained: {retain}\")\n",
    "    \n",
    "    level += 1\n",
    "    newsets = mingle(retain, level)\n",
    "    print(f\"New candidates for level {level}: {newsets}\")\n",
    "    \n",
    "    if len(newsets) != 0 and level < len(items) + 1:\n",
    "        apriori(level, transactions, newsets, minsup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run A-Priori with minSup = 50%\n",
    "print(\"A-PRIORI ALGORITHM\")\n",
    "print(\"minSup = 50% (0.5)\")\n",
    "print(\"\\nTransactions:\")\n",
    "for i, t in enumerate(transactions):\n",
    "    print(f\"  Basket {i+1}: {t}\")\n",
    "\n",
    "items = {'iphoneX', 'S8', 'LG55', 'S9', 'S2'}\n",
    "apriori(1, transactions, items, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 4: Recommendation Systems\n",
    "\n",
    "**Recommendation systems** predict what items a user might like based on:\n",
    "- **Collaborative Filtering**: Find similar users/items\n",
    "- **Content-Based**: Match item features to user preferences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 The Utility Matrix\n",
    "\n",
    "A **utility matrix** stores user-item interactions (ratings, purchases, etc.):\n",
    "\n",
    "| User | Python | R | MATLAB | Java |\n",
    "|------|--------|---|--------|------|\n",
    "| Douglas | 5 | 4 | 3 | - |\n",
    "| Johannes | 4 | 5 | - | 2 |\n",
    "| Maurizio | 5 | 4 | - | - |\n",
    "| Tong | 3 | - | 5 | 4 |\n",
    "\n",
    "**Challenge**: The matrix is very sparse! (Most users haven't rated most items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Jaccard Similarity\n",
    "\n",
    "For binary data (bought/didn't buy):\n",
    "\n",
    "$$J(X, Y) = \\frac{|X \\cap Y|}{|X \\cup Y|}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(set1, set2):\n",
    "    \"\"\"Calculate Jaccard similarity between two sets.\"\"\"\n",
    "    intersection = len(set1 & set2)\n",
    "    union = len(set1 | set2)\n",
    "    return intersection / union if union > 0 else 0\n",
    "\n",
    "# Example from lecture\n",
    "douglas = {'Python', 'R', 'MATLAB'}\n",
    "maurizio = {'Python', 'R'}\n",
    "\n",
    "sim = jaccard_similarity(douglas, maurizio)\n",
    "print(f\"Douglas knows: {douglas}\")\n",
    "print(f\"Maurizio knows: {maurizio}\")\n",
    "print(f\"\\nJaccard(Douglas, Maurizio) = |{douglas & maurizio}| / |{douglas | maurizio}|\")\n",
    "print(f\"                           = {len(douglas & maurizio)} / {len(douglas | maurizio)}\")\n",
    "print(f\"                           = {sim:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Cosine Similarity\n",
    "\n",
    "For rating data (vectors):\n",
    "\n",
    "$$cos(\\theta) = \\frac{X \\cdot Y}{\\|X\\| \\|Y\\|} = \\frac{\\sum_{i} x_i y_i}{\\sqrt{\\sum_{i} x_i^2} \\sqrt{\\sum_{i} y_i^2}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim(vec1, vec2):\n",
    "    \"\"\"Calculate cosine similarity between two vectors.\"\"\"\n",
    "    # scipy.distance.cosine returns DISTANCE, so we subtract from 1\n",
    "    return 1 - cosine(vec1, vec2)\n",
    "\n",
    "# Example with rating vectors\n",
    "user_a = np.array([5, 3, 0, 1])  # Ratings for 4 items\n",
    "user_b = np.array([4, 0, 0, 1])\n",
    "\n",
    "sim = cosine_sim(user_a, user_b)\n",
    "print(f\"User A ratings: {user_a}\")\n",
    "print(f\"User B ratings: {user_b}\")\n",
    "print(f\"\\nCosine similarity: {sim:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Collaborative Filtering Example\n",
    "\n",
    "Let's build a simple user-based recommendation system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ratings data\n",
    "ratings = pd.read_csv('data/ratings.csv')\n",
    "ratings = ratings[:5000]  # Sample for speed\n",
    "\n",
    "noMovies = len(ratings['movieId'].unique())\n",
    "noUsers = len(ratings['userId'].unique())\n",
    "\n",
    "print(f\"Dataset: {noMovies} movies rated by {noUsers} users\")\n",
    "print(ratings.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create utility matrix\n",
    "utility = np.zeros(shape=(noUsers, noMovies))\n",
    "\n",
    "# Map movie IDs to sequential indices\n",
    "movieIds = {mid: idx for idx, mid in enumerate(ratings['movieId'].unique())}\n",
    "\n",
    "# Populate the matrix\n",
    "for _, row in ratings.iterrows():\n",
    "    uid = int(row['userId']) - 1\n",
    "    mid = movieIds[row['movieId']]\n",
    "    utility[uid, mid] = row['rating']\n",
    "\n",
    "print(f\"Utility matrix shape: {utility.shape}\")\n",
    "print(f\"Sparsity: {(utility == 0).sum() / utility.size * 100:.1f}% empty\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar_users(user_id, utility_matrix, min_similarity=0.5):\n",
    "    \"\"\"Find users similar to the given user.\"\"\"\n",
    "    similar_users = []\n",
    "    user_ratings = utility_matrix[user_id]\n",
    "    \n",
    "    for other_id in range(len(utility_matrix)):\n",
    "        if user_id != other_id:\n",
    "            other_ratings = utility_matrix[other_id]\n",
    "            # Only compare if both users have rated at least one item\n",
    "            if np.any(user_ratings) and np.any(other_ratings):\n",
    "                sim = cosine_sim(user_ratings, other_ratings)\n",
    "                if sim > min_similarity:\n",
    "                    similar_users.append((other_id, sim))\n",
    "    \n",
    "    return sorted(similar_users, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Find similar users for user 0\n",
    "similar = find_similar_users(0, utility, min_similarity=0.3)\n",
    "print(f\"Users similar to User 0:\")\n",
    "for uid, sim in similar[:5]:\n",
    "    print(f\"  User {uid}: similarity = {sim:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_movies(user_id, utility_matrix, similar_users, n_recommendations=5):\n",
    "    \"\"\"Recommend movies based on similar users' ratings.\"\"\"\n",
    "    user_ratings = utility_matrix[user_id]\n",
    "    recommendations = []\n",
    "    \n",
    "    # For each movie the user hasn't rated\n",
    "    for movie_id in range(utility_matrix.shape[1]):\n",
    "        if user_ratings[movie_id] == 0:  # Not rated\n",
    "            scores = []\n",
    "            for sim_user_id, sim in similar_users:\n",
    "                sim_rating = utility_matrix[sim_user_id, movie_id]\n",
    "                if sim_rating > 0:  # Similar user has rated this movie\n",
    "                    scores.append(sim_rating * sim)  # Weight by similarity\n",
    "            \n",
    "            if scores:\n",
    "                avg_score = sum(scores) / len(scores)\n",
    "                recommendations.append((movie_id, avg_score))\n",
    "    \n",
    "    return sorted(recommendations, key=lambda x: x[1], reverse=True)[:n_recommendations]\n",
    "\n",
    "# Get recommendations for user 0\n",
    "similar = find_similar_users(0, utility, min_similarity=0.3)\n",
    "recs = recommend_movies(0, utility, similar, n_recommendations=5)\n",
    "\n",
    "print(f\"\\nTop 5 recommendations for User 0:\")\n",
    "for movie_idx, score in recs:\n",
    "    print(f\"  Movie index {movie_idx}: predicted score = {score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Matrix Factorization with NMF\n",
    "\n",
    "**Non-negative Matrix Factorization** decomposes the utility matrix:\n",
    "\n",
    "$$M \\approx U \\times V^T$$\n",
    "\n",
    "Where:\n",
    "- $M$ is the original utility matrix (users x items)\n",
    "- $U$ is user features (users x latent factors)\n",
    "- $V^T$ is item features (latent factors x items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NMF decomposition\n",
    "n_components = 20  # Number of latent factors\n",
    "\n",
    "nmf = NMF(n_components=n_components, init='random', random_state=42, max_iter=500)\n",
    "U = nmf.fit_transform(utility)\n",
    "V_T = nmf.components_\n",
    "\n",
    "print(f\"Original matrix shape: {utility.shape}\")\n",
    "print(f\"U (user features): {U.shape}\")\n",
    "print(f\"V^T (item features): {V_T.shape}\")\n",
    "\n",
    "# Reconstruct matrix\n",
    "M_reconstructed = np.dot(U, V_T)\n",
    "print(f\"\\nReconstruction error: {np.sum(utility - M_reconstructed):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Summary\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "| Topic | Key Concept | Application |\n",
    "|-------|-------------|-------------|\n",
    "| **Clustering** | Group similar items without labels | Customer segmentation (RFM) |\n",
    "| **Sentiment Analysis** | Determine emotional tone of text | Product review analysis |\n",
    "| **Frequent Itemsets** | Find items bought together | \"Customers also bought\" |\n",
    "| **Recommendations** | Predict user preferences | Personalized suggestions |\n",
    "\n",
    "## Quick Quiz\n",
    "\n",
    "1. K-means requires specifying ___ in advance (Answer: K, number of clusters)\n",
    "2. VADER is designed for analyzing ___ text (Answer: social media)\n",
    "3. Support measures how ___ an itemset appears (Answer: frequently)\n",
    "4. Lift > 1 indicates items are ___ (Answer: dependent/associated)\n",
    "5. Collaborative filtering finds similar ___ (Answer: users or items)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
